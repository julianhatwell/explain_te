{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up notebook and load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load what we need\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.reproducible as rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several datasets are available as pre-prepared containers that hold the data and some meta-data that is used in the algorithm\n",
    "Any dataset can be turned into a container by invoking the constructor found in the file structures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets might be down-sampled to make them easier to work with\n",
    "# the full sets are available too\n",
    "# this is a list of constructors that will be used in the benchmarking\n",
    "\n",
    "# demo datasets that ship with package. all from UCI unless stated otherwise\n",
    "# ds.adult_data, ds.adult_samp_data, ds.adult_small_samp_data Large dataset ships with manageable sub samples\n",
    "# ds.bankmark_data, ds.bankmark_samp_data\n",
    "# ds.car_data\n",
    "# ds.cardio_data this is the cardiotocography dataset\n",
    "# ds.credit_data\n",
    "# ds.german_data\n",
    "# ds.lending_data, ds.lending_samp_data, ds.lending_small_samp_data, ds.lending_tiny_samp_data from Kaggle. see datasets_from_source file for links\n",
    "# ds.nursery_data, ds.nursery_samp_data\n",
    "# ds.rcdv_data, ds.rcdv_samp_data from US government see datasets_from_source file for links\n",
    "\n",
    "if False:\n",
    "    rp.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of one dataset\n",
    "# note: random_state propagates through other functions and is easily updated to allow alternative runs\n",
    "if False:\n",
    "    ds.cardio(random_state=123, project_dir=project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardising train-test splitting\n",
    "Some methods are not available in Python. We want to maintain the same dataset splits no matter which platform. So, the train test data is split with the one-time random seed and the splits are saved to csv in the project folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# writes external files\n",
    "if False:\n",
    "    rp.export_data_splits(datasets=rp.datasets, project_dir=project_dir, random_state_splits=random_state_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Runs\n",
    "Loop through datasets, actioning the functions in the package to execute a round of experiments and test evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Optional Memory and Computation Cost Management\n",
    "CHIRPS is time economical but memory intensive to compute for lots of instances at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing\n",
    "Scikit takes care of parallel for the RF construction.\n",
    "We can parallelise the following:\n",
    "1. the walk of instances down each tree to collect the paths. The paths for many instances are returned in a single array. This parallelises across trees.\n",
    "2. building CHIRPS and the final explanation (rule). This is a search optimisation and we can parallelise each instance.\n",
    "\n",
    "This is expecially effective when running batches. For single instances, set both to false to avoid spinning up the parallel infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing unseen data\n",
    "\n",
    "Again note:\n",
    "test set has never been \"seen\" by random forest during training\n",
    "test set has been only used to assess model (random forest) accuracy - no additional tuning after this\n",
    "test set has not be involved in generating the explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "The memory space requirements for all the paths can be reduced by dividing the test set into batches. However this does take longer as there is an overhead to instantiate all the required objects, especially if coupled with parallel processing.\n",
    "Best compromise could be a small number of larger batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data and Forest prep\n",
    "Use the random state splits to do a one-off data split.\n",
    "Fit the RF to training data, using the iterating random state.\n",
    "Save the performance metrics on the test set for later review.\n",
    "\n",
    "### 2. Prepare Unseen Data and Predictions\n",
    "Important to note:\n",
    "Test set never \"seen\" by RF during training.\n",
    "test set not involved in generating the explainer.\n",
    "Test set used to evaluate model (random forest) accuracy beyond OOBE scores - no additional tuning based on these results.\n",
    "Test set used to evaluate explanation scores by leave-one-out method removing the specific instance we're explaining.\n",
    "\n",
    "Important to note:\n",
    "We will explain predictions directly from the trained RF. Explanation system makes no compromise on model accuracy.\n",
    "\n",
    "### 3. CHIRPS algorithm\n",
    "1. Extract tree prediction paths\n",
    "2. Freqent pattern mining of paths\n",
    "3. Score and sort mined path segments\n",
    "4. Merge path segments into one rule\n",
    "\n",
    "#### CHIRPS 1\n",
    "Fit a forest_walker object to the dataset and decision forest. This is a wrapper that will extract the paths of all the given instances. Its main method delivers the instance paths for the remaining steps of the algorithm as a new object: a batch_paths_container. It can also report interesting statistics (treating the forest as a set of random tree-structured variables).\n",
    "\n",
    "#### CHIRPS 2-4\n",
    "A batch_CHIRPS_container is fitted with the batch_paths_container returned by the forest walker, and with a sample of data. For CHIRPS, we prefer a large sample. The whole training set or other representative sample will do. This is a wrapper object will execute steps 2-4 on all each the instance-paths in the batch_paths_container.\n",
    "\n",
    "Important to note:\n",
    "true_divide warnings are OK! It just means that a continuous variable is unbounded on one side i.e. no greater/less than inequality is used in the specific CHIRPS explanation.\n",
    "\n",
    "Important note: \n",
    "Here we are using the training set to create the explainers. We could use a different dataset as long as it is representative of the training set that built the decision forest. Most important that we don't use the dataset that we wish to explain, so never use the test set, for example.\n",
    "\n",
    "### 4. Evaluating CHIRPS Explanations\n",
    "Test set has been used to create an explainer *one instance at a time* and the rest of test set was not \"seen\" during this construction. To score each explainer, we use test set, leaving out the individual instance being explained. The data_split_container (tt) has a convenience funtion for doing this. All the results are saved to csv files in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing car data and model for car with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 1.0000\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 1.0000\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600, 'random_state': 123}\n",
      "\n",
      "\n",
      "Beginning benchmark for car data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'support_paths': 0.01, 'alpha_paths': 0.0, 'disc_path_bins': 8, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'kldiv', 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 518 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 10.1538 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 518 instances... (please wait)\n",
      "CHIRPS time elapsed: 167.6082 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\car\\AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 35.1465 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHIRPS default set up\n",
    "merging_bootstraps = 20\n",
    "pruning_bootstraps = 20\n",
    "delta = 0.1\n",
    "\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=True\n",
    "\n",
    "n_instances = 10000\n",
    "\n",
    "# model = 'RandomForest'\n",
    "# model = 'AdaBoost1'\n",
    "model = 'AdaBoost2'\n",
    "# model = 'GBM'\n",
    "\n",
    "do_Anchors = True\n",
    "do_dfrgTrs = True\n",
    "\n",
    "datasets = rp.datasets # here can opt for just one, e.g. [rp.datasets[0]] (as an iterator)\n",
    "start_instance = 0 # here can opt to start at a specific instance\n",
    "\n",
    "project_dir = 'V:\\\\whiteboxing\\\\tests' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "# project_dir = 'C:\\\\Users\\\\Crutt\\\\Documents\\\\whiteboxing\\\\tests'\n",
    "random_state_splits = 123 # change this if you want to try different splits of the data into test / train\n",
    "random_state_rf = 123 # change this if you want to try with different forest construction\n",
    "random_state_exp = 123 # change this if you want to try with different runs of the explainer algorithm (affects bootstrap eval)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "tuning = {'grid' : None, 'override' : False}\n",
    "if model == 'RandomForest':\n",
    "    tuning.update({'grid' : None}) # defaults to n_trees [200, 400, ..., 1600]\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=random_state_rf,\n",
    "                                           random_state_splits=random_state_splits,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "\n",
    "    kwargs = {'support_paths' : 0.1, 'alpha_paths' : 0.5, 'disc_path_bins' : 4,\n",
    "             'score_func' : 1, 'weighting' : 'chisq',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    " \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'chirps_explanation_async' : chirps_explanation_async}\n",
    "    \n",
    "    rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "    \n",
    "    if do_Anchors:\n",
    "        control = {'method' : 'Anchors',\n",
    "                   'model' : model,\n",
    "                   'n_instances' : n_instances,\n",
    "                   'random_state' : random_state_exp}\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "        \n",
    "    if do_dfrgTrs:\n",
    "        control = {'method' : 'defragTrees',\n",
    "                    'Kmax' : 1, 'restart' : 1, 'maxitr' : 1,\n",
    "                    'model' : model,\n",
    "                    'n_instances' : n_instances,\n",
    "                    'random_state' : random_state_exp}\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "            \n",
    "elif model == \"AdaBoost1\":\n",
    "    algo = 'SAMME'\n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=123, random_state_splits=123,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "             'support_paths' : 0.1, 'alpha_paths' : 0.0,\n",
    "             'disc_path_bins' : 4, 'disc_path_eqcounts' : True,\n",
    "             'score_func' : 1, 'weighting' : 'chisq',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    " \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'chirps_explanation_async' : chirps_explanation_async}\n",
    "    \n",
    "    rp.do_benchmarking(benchmark_items, verbose=True, **control)\n",
    "    \n",
    "elif model == 'AdaBoost2':\n",
    "    algo = 'SAMME.R'\n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=123, random_state_splits=123,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "                 'support_paths' : 0.01, 'alpha_paths' : 0.0,\n",
    "                 'disc_path_bins' : 8, 'disc_path_eqcounts' : True,\n",
    "                 'score_func' : 1, 'weighting' : 'kldiv',\n",
    "                 'merging_bootstraps' : merging_bootstraps,\n",
    "                 'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    "    \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'chirps_explanation_async' : chirps_explanation_async}\n",
    "    \n",
    "    rp.do_benchmarking(benchmark_items, verbose=True, **control)\n",
    "    \n",
    "else: # GBM\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=123, random_state_splits=123,\n",
    "                                           start_instance=start_instance, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing adult_small_samp data and model for adult_small_samp with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.8473\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.8473\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing bankmark_samp data and model for bankmark_samp with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.9211\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9211\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing car data and model for car with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 1.0000\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 1.0000\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing cardio data and model for cardio with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9462\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9462\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing credit data and model for credit with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.8737\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.8737\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing german data and model for german with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.7471\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.7471\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing lending_tiny_samp data and model for lending_tiny_samp with random state = 123\n",
      "Split data into main train-test and build forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9898\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9898\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing nursery_samp data and model for nursery_samp with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.9338\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.9338\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing rcdv_samp data and model for rcdv_samp with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.6437\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model for Anchors\n",
      "using previous tuning parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.6437\n",
      "Best parameters:{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "\n",
      "Beginning benchmark for adult_small_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 733 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 2.4302 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 733 instances... (please wait)\n",
      "CHIRPS time elapsed: 270.5437 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\adult_small_samp\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 26.5359 seconds\n",
      "\n",
      "Beginning benchmark for bankmark_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 680 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 7.3636 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 680 instances... (please wait)\n",
      "CHIRPS time elapsed: 323.5263 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\bankmark_samp\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 43.3467 seconds\n",
      "\n",
      "Beginning benchmark for car data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 518 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 5.1591 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 518 instances... (please wait)\n",
      "CHIRPS time elapsed: 147.6095 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\car\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 36.2703 seconds\n",
      "\n",
      "Beginning benchmark for cardio data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 638 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 24.0187 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 638 instances... (please wait)\n",
      "CHIRPS time elapsed: 816.1685 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\cardio\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 156.6303 seconds\n",
      "\n",
      "Beginning benchmark for credit data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 207 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 1.4137 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 207 instances... (please wait)\n",
      "CHIRPS time elapsed: 92.7354 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\credit\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 4.2839 seconds\n",
      "\n",
      "Beginning benchmark for german data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 300 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 10.7987 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 300 instances... (please wait)\n",
      "CHIRPS time elapsed: 436.4096 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\german\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 45.9792 seconds\n",
      "\n",
      "Beginning benchmark for lending_tiny_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 632 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 7.7010 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 632 instances... (please wait)\n",
      "CHIRPS time elapsed: 348.2786 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\lending_tiny_samp\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 49.0196 seconds\n",
      "\n",
      "Beginning benchmark for nursery_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 778 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 10.4749 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 778 instances... (please wait)\n",
      "CHIRPS time elapsed: 385.3065 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\nursery_samp\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 82.5842 seconds\n",
      "\n",
      "Beginning benchmark for rcdv_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.05, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 566 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 1.5331 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 566 instances... (please wait)\n",
      "CHIRPS time elapsed: 288.3122 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\rcdv_samp\\ada2_sensitivity\\wcts_majoritysp_0.05_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 16.3684 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning benchmark for adult_small_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.02, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 733 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 2.4396 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 733 instances... (please wait)\n",
      "CHIRPS time elapsed: 289.2198 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\adult_small_samp\\ada2_sensitivity\\wcts_majoritysp_0.02_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 23.9083 seconds\n",
      "\n",
      "Beginning benchmark for bankmark_samp data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.02, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 680 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 7.0168 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 680 instances... (please wait)\n",
      "CHIRPS time elapsed: 342.1330 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\bankmark_samp\\ada2_sensitivity\\wcts_majoritysp_0.02_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 46.1109 seconds\n",
      "\n",
      "Beginning benchmark for car data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.02, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 518 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 5.8902 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 518 instances... (please wait)\n",
      "CHIRPS time elapsed: 149.9373 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\car\\ada2_sensitivity\\wcts_majoritysp_0.02_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 35.9896 seconds\n",
      "\n",
      "Beginning benchmark for cardio data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.02, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 638 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 24.0258 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 638 instances... (please wait)\n",
      "CHIRPS time elapsed: 873.0368 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\cardio\\ada2_sensitivity\\wcts_majoritysp_0.02_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 156.2146 seconds\n",
      "\n",
      "Beginning benchmark for credit data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.02, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 207 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 1.5720 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 207 instances... (please wait)\n",
      "CHIRPS time elapsed: 103.5755 seconds\n",
      "CHIRPS with async = True\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\credit\\ada2_sensitivity\\wcts_majoritysp_0.02_ap_0.0_dpb_4_dpeq_True_sf_1_w_chisq_AdaBoost2_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 4.5327 seconds\n",
      "\n",
      "Beginning benchmark for german data.\n",
      "Hyper-parameter settings: {'paths_lengths_threshold': 5, 'alpha_paths': 0.0, 'disc_path_bins': 4, 'disc_path_eqcounts': True, 'score_func': 1, 'weighting': 'chisq', 'support_paths': 0.02, 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'which_trees': 'majority', 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 300 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 11.2500 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 300 instances... (please wait)\n"
     ]
    }
   ],
   "source": [
    "# load what we need\n",
    "import numpy as np\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.reproducible as rp\n",
    "\n",
    "# CHIRPS default set up\n",
    "merging_bootstraps = 20\n",
    "pruning_bootstraps = 20\n",
    "delta = 0.1 # prune rule terms if loss of precision no greater than delta\n",
    "\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=True\n",
    "\n",
    "n_instances = 10000\n",
    "\n",
    "# model = 'RandomForest'\n",
    "# model = 'AdaBoost1'\n",
    "model = 'AdaBoost2'\n",
    "# model = 'GBM'\n",
    "\n",
    "do_Anchors = False\n",
    "do_dfrgTrs = False\n",
    "\n",
    "datasets = rp.datasets # here can opt for just one, e.g. [rp.datasets[0]] (as an iterator)\n",
    "start_instance = 0 # here can opt to start at a specific instance\n",
    "\n",
    "project_dir = 'V:\\\\whiteboxing\\\\tests' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "# project_dir = 'C:\\\\Users\\\\Crutt\\\\Documents\\\\whiteboxing\\\\tests'\n",
    "random_state_splits = 123 # change this if you want to try different splits of the data into test / train\n",
    "random_state_rf = 123 # change this if you want to try with different forest construction\n",
    "random_state_exp = 123 # change this if you want to try with different runs of the explainer algorithm (affects bootstrap eval)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "tuning = {'grid' : None, 'override' : False}\n",
    "\n",
    "# for troubleshooting, can isolate an instance and do like\n",
    "# bi_copy['cardio']['main']['ds_container'].current_row_test = 110\n",
    "\n",
    "if model == 'RandomForest':\n",
    "    tuning.update({'grid' : None}) # defaults to n_trees [200, 400, ..., 1600]\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=random_state_rf,\n",
    "                                           random_state_splits=random_state_splits,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "    \n",
    "    alpha_paths = np.tile([0.9, 0.5, 0.1], 24)\n",
    "    disc_path_bins = np.tile(np.repeat([4, 8], 3), 12)\n",
    "    score_func = np.tile(np.repeat([5, 3, 1], 6), 4)\n",
    "    support_paths = np.tile(np.repeat([0.1, 0.05], 18), 2)\n",
    "    weighting = np.repeat(['chisq', 'nothing'], 36)\n",
    "\n",
    "    kwargs_grid = {k : {'alpha_paths' : ap, 'disc_path_bins' : dpb, 'disc_path_eqcounts' : False,\n",
    "                        'score_func' : sf, 'weighting' : w, 'support_paths' : sp,\n",
    "                        'merging_bootstraps' : merging_bootstraps,\n",
    "                        'pruning_bootstraps' : pruning_bootstraps,\n",
    "                        'delta' : delta} \n",
    "        for k, ap, dpb, sf, w, sp in zip(range(72), alpha_paths, disc_path_bins, score_func, weighting, support_paths)}\n",
    "    \n",
    "    \n",
    "    for kwargs in kwargs_grid:\n",
    "        bi_copy = rp.deepcopy(benchmark_items) # to avoid running down the internal counters\n",
    "        control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                    'n_instances' : n_instances,\n",
    "                    'random_state' : random_state_exp,\n",
    "                    'kwargs' : kwargs_grid[kwargs],\n",
    "                    'forest_walk_async' : forest_walk_async,\n",
    "                    'chirps_explanation_async' : chirps_explanation_async,\n",
    "                    'save_sensitivity_path' : 'rf_sensitivity'}\n",
    "\n",
    "\n",
    "        rp.do_benchmarking(bi_copy, verbose, **control)\n",
    "        \n",
    "elif model in ('AdaBoost1', 'AdaBoost2'):\n",
    "    if model == 'AdaBoost1':\n",
    "        algo = 'SAMME'\n",
    "        save_sensitivity_path = 'ada1_sensitivity'\n",
    "        n_kwargs = 48\n",
    "    else:\n",
    "        algo = 'SAMME.R'\n",
    "        save_sensitivity_path = 'ada2_sensitivity'\n",
    "        n_kwargs = 96\n",
    "        \n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=123, random_state_splits=123,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "    \n",
    "    disc_path_bins = np.tile(np.tile(np.repeat([4, 8], 3), 8), 2)\n",
    "    disc_path_eqcounts = np.tile(np.tile(np.repeat([True, False], 6), 4), 2)\n",
    "    support_paths = np.tile(np.tile([0.05, 0.02, 0.01], 16), 2)\n",
    "    weighting = np.tile(np.repeat(['chisq', 'kldiv', 'lodds', 'nothing'], 12), 2)\n",
    "    which_trees = np.repeat(['majority', 'conf_weighted'], 48)\n",
    "\n",
    "    kwargs_grid = {k : {'paths_lengths_threshold' : 5, 'alpha_paths' : 0.0,\n",
    "                        'disc_path_bins' : dpb, 'disc_path_eqcounts' : dpeq,\n",
    "                        'score_func' : 1, 'weighting' : w, 'support_paths' : sp,\n",
    "                        'merging_bootstraps' : merging_bootstraps,\n",
    "                        'pruning_bootstraps' : pruning_bootstraps,\n",
    "                        'which_trees' : wchtr,\n",
    "                        'delta' : delta} \n",
    "    for k, dpb, dpeq, w, sp, wchtr \\\n",
    "                   in zip(range(n_kwargs), disc_path_bins, disc_path_eqcounts, weighting, support_paths, which_trees)}\n",
    "\n",
    "    for kwargs in kwargs_grid: # range(18, 48): \n",
    "        bi_copy = rp.deepcopy(benchmark_items) # to avoid running down the internal counters\n",
    "        control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                    'n_instances' : n_instances,\n",
    "                    'random_state' : random_state_exp,\n",
    "                    'kwargs' : kwargs_grid[kwargs],\n",
    "                    'forest_walk_async' : forest_walk_async,\n",
    "                    'chirps_explanation_async' : chirps_explanation_async,\n",
    "                    'save_sensitivity_path' : save_sensitivity_path}\n",
    " \n",
    "        rp.do_benchmarking(bi_copy, verbose, **control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
