{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up notebook and load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for notebook plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# load what we need\n",
    "import time\n",
    "import timeit\n",
    "import CHIRPS.structures as strcts\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.routines as rt\n",
    "import CHIRPS.reproducible as rp\n",
    "\n",
    "# demo datasets that ship with package. all from UCI unless stated otherwise\n",
    "# ds.adult_data, ds.adult_samp_data, ds.adult_small_samp_data Large dataset ships with manageable sub samples\n",
    "# ds.bankmark_data, ds.bankmark_samp_data\n",
    "# ds.car_data\n",
    "# ds.cardio_data this is the cardiotocography dataset\n",
    "# ds.credit_data\n",
    "# ds.german_data\n",
    "# ds.lending_data, ds.lending_samp_data, ds.lending_small_samp_data, ds.lending_tiny_samp_data from Kaggle. see datasets_from_source file for links\n",
    "# ds.nursery_data, ds.nursery_samp_data\n",
    "# ds.rcdv_data, ds.rcdv_samp_data from US government see datasets_from_source file for links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common config - can be ommitted if defaults are OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'V:\\\\whiteboxing' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "random_state_splits = 123 # one off for splitting the data into test / train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several datasets are available as pre-prepared containers that hold the data and some meta-data that is used in the algorithm\n",
    "Any dataset can be turned into a container by invoking the constructor found in the file structures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function CHIRPS.datasets.adult_small_samp_data>,\n",
       " <function CHIRPS.datasets.bankmark_samp_data>,\n",
       " <function CHIRPS.datasets.car_data>,\n",
       " <function CHIRPS.datasets.cardio_data>,\n",
       " <function CHIRPS.datasets.credit_data>,\n",
       " <function CHIRPS.datasets.german_data>,\n",
       " <function CHIRPS.datasets.lending_tiny_samp_data>,\n",
       " <function CHIRPS.datasets.nursery_samp_data>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets might be down-sampled to make them easier to work with\n",
    "# the full sets are available too\n",
    "# this is a list of constructors that will be used in the benchmarking\n",
    "rp.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CHIRPS.structures.data_container at 0x1baac544978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of one dataset\n",
    "# note: random_state propagates through other functions and is easily updated to allow alternative runs\n",
    "ds.cardio_data(random_state=123, project_dir=project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardising train-test splitting\n",
    "Some methods are not available in Python. We want to maintain the same dataset splits no matter which platform. So, the train test data is split with the one-time random seed and the splits are saved to csv in the project folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported train-test data for 8 datasets.\n"
     ]
    }
   ],
   "source": [
    "# writes external files\n",
    "rp.export_data_splits(datasets=rp.datasets, project_dir=project_dir, random_state_splits=random_state_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Runs\n",
    "Loop through datasets, actioning the functions in the package to execute a round of experiments and test evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Optional Memory and Computation Cost Management\n",
    "CHIRPS is time economical but memory intensive to compute for lots of instances at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing\n",
    "Scikit takes care of parallel for the RF construction.\n",
    "We can parallelise:\n",
    "1. the walk of instances down each tree to collect the paths.\n",
    "2. building CHIRPS and the final explanation (rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control for async processes - each tree walk can be done in its own core\n",
    "# and so can each explanation (e.g. rule conditions merge by hill-climbing)\n",
    "# these will default to false if not passed explicitly to the explainer function\n",
    "# on a multi-core machine there should be a good speed up for large batches\n",
    "# when the batch_size advantage exceeds the overhead of setting up multi-processing\n",
    "# timings will be printed to screen so you can see if it helps\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing unseen data\n",
    "\n",
    "Again note:\n",
    "test set has never been \"seen\" by random forest during training\n",
    "test set has been only used to assess model (random forest) accuracy - no additional tuning after this\n",
    "test set has not be involved in generating the explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "The memory space requirements for all the paths can be reduced by dividing the test set into batches. However this does take longer as there is an overhead to instantiate all the required objects, especially if coupled with parallel processing.\n",
    "Best compromise could be a small number of larger batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of instances can be controlled by\n",
    "# batch_size - how many instances to explain at one time\n",
    "# set these larger than the size of the test set and it will simply run the whole test set in one batch. Better do option 2\n",
    "batch_size = 2\n",
    "# how many instances to explain in total from a test/unseen set\n",
    "n_instances = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data and Forest prep\n",
    "Use the random state splits to do a one-off data split.\n",
    "Fit the RF to training data, using the iterating random state.\n",
    "Save the performance metrics on the test set for later review.\n",
    "\n",
    "### 2. Prepare Unseen Data and Predictions\n",
    "Important to note:\n",
    "Test set never \"seen\" by RF during training.\n",
    "test set not involved in generating the explainer.\n",
    "Test set used to evaluate model (random forest) accuracy beyond OOBE scores - no additional tuning based on these results.\n",
    "Test set used to evaluate explanation scores by leave-one-out method removing the specific instance we're explaining.\n",
    "\n",
    "Important to note:\n",
    "We will explain predictions directly from the trained RF. Explanation system makes no compromise on model accuracy.\n",
    "\n",
    "### 3. CHIRPS algorithm\n",
    "1. Extract Tree Prediction Paths\n",
    "2. Freqent pattern mining of paths\n",
    "3. Score and sort mined path segments\n",
    "4. Merge path segments into one rule\n",
    "\n",
    "#### CHIRPS 1\n",
    "Fit a forest_walker object to the dataset and decision forest. This is a wrapper that will extract the paths of all the given instances. Its main method delivers the instance paths for the remaining steps of the algorithm as a new object: a batch_paths_container. It can also report interesting statistics (treating the forest as a set of random tree-structured variables).\n",
    "\n",
    "#### CHIRPS 2-4\n",
    "A batch_CHIRPS_container is fitted with the batch_paths_container returned by the forest walker, and with a sample of data. For CHIRPS, we prefer a large sample. The whole training set or other representative sample will do. This is a wrapper object will execute steps 2-4 on all each the instance-paths in the batch_paths_container.\n",
    "\n",
    "Important to note:\n",
    "true_divide warnings are OK! It just means that a continuous variable is unbounded on one side i.e. no greater/less than inequality is used in the specific CHIRPS explanation.\n",
    "\n",
    "Important note: \n",
    "Here we are using the training set to create the explainers. We could use a different dataset as long as it is representative of the training set that built the decision forest. Most important that we don't use the dataset that we wish to explain, so never use the test set, for example.\n",
    "\n",
    "### 4. Evaluating CHIRPS Explanations\n",
    "Test set has been used to create an explainer *one instance at a time* and the rest of test set was not \"seen\" during this construction. To score each explainer, we use test set, leaving out the individual instance being explained. The data_split_container (tt) has a convenience funtion for doing this. All the results are saved to csv files in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for adult_small_samp_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.2306 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.2916 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0283 seconds\n",
      "\n",
      "\n",
      "Running experiment for bankmark_samp_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.4378 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.3702 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0540 seconds\n",
      "\n",
      "\n",
      "Running experiment for car_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.1875 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n",
      "CHIRPS time elapsed: 0.5497 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0214 seconds\n",
      "\n",
      "\n",
      "Running experiment for cardio_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.4320 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.6077 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0392 seconds\n",
      "\n",
      "\n",
      "Running experiment for credit_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.3994 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.3121 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0338 seconds\n",
      "\n",
      "\n",
      "Running experiment for german_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.2240 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.4213 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0494 seconds\n",
      "\n",
      "\n",
      "Running experiment for lending_tiny_samp_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.7110 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 1.1822 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0740 seconds\n",
      "\n",
      "\n",
      "Running experiment for nursery_samp_data with random state = 123\n",
      "\n",
      "Split data into train-test and build RF\n",
      "using previous tuning parameters\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.6595 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n",
      "CHIRPS time elapsed: 0.6341 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.1081 seconds\n",
      "\n",
      "\n",
      "Running experiment for adult_small_samp_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.064868337316554\n",
      "Out of Bag Accuracy Score: 0.8455236980690463\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 2.070325462437424\n",
      "Out of Bag Accuracy Score: 0.844938560561732\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 3.25349911631951\n",
      "Out of Bag Accuracy Score: 0.8455236980690463\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.0071839227240815\n",
      "Out of Bag Accuracy Score: 0.8385020479812756\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.9628069970908584\n",
      "Out of Bag Accuracy Score: 0.839672322995904\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 2.8947035058167856\n",
      "Out of Bag Accuracy Score: 0.8379169104739613\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.6548402915033833\n",
      "Out of Bag Accuracy Score: 0.8455236980690463\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 3.393271769157778\n",
      "Out of Bag Accuracy Score: 0.8461088355763604\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 5.160659482243162\n",
      "Out of Bag Accuracy Score: 0.8461088355763604\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.2506359434708898\n",
      "Out of Bag Accuracy Score: 0.8361614979520188\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 2.419682127419456\n",
      "Out of Bag Accuracy Score: 0.8420128730251609\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 3.587502997182199\n",
      "Out of Bag Accuracy Score: 0.8402574605032183\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.8461\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.4594 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.4277 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0689 seconds\n",
      "\n",
      "\n",
      "Running experiment for bankmark_samp_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.1535155219483926\n",
      "Out of Bag Accuracy Score: 0.9148264984227129\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 2.244431250499531\n",
      "Out of Bag Accuracy Score: 0.9141955835962146\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 3.4299912323031094\n",
      "Out of Bag Accuracy Score: 0.9123028391167193\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.047445018608073\n",
      "Out of Bag Accuracy Score: 0.9129337539432176\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 2.1161458430359943\n",
      "Out of Bag Accuracy Score: 0.9110410094637224\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 3.1480205376839763\n",
      "Out of Bag Accuracy Score: 0.910410094637224\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.5758805405804992\n",
      "Out of Bag Accuracy Score: 0.9135646687697161\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 3.1623418103761765\n",
      "Out of Bag Accuracy Score: 0.9148264984227129\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 4.802810650798676\n",
      "Out of Bag Accuracy Score: 0.9154574132492114\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.2842146000481875\n",
      "Out of Bag Accuracy Score: 0.9123028391167193\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 2.5913502072938286\n",
      "Out of Bag Accuracy Score: 0.9129337539432176\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 3.855514184258652\n",
      "Out of Bag Accuracy Score: 0.910410094637224\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.9155\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.6769 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.5501 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0801 seconds\n",
      "\n",
      "\n",
      "Running experiment for car_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 0.7874389371075523\n",
      "Out of Bag Accuracy Score: 0.9752066115702479\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 1.5659214825400198\n",
      "Out of Bag Accuracy Score: 0.9743801652892562\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 2.371384376673248\n",
      "Out of Bag Accuracy Score: 0.9743801652892562\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 0.7332540895386757\n",
      "Out of Bag Accuracy Score: 0.9694214876033058\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.45720234149789\n",
      "Out of Bag Accuracy Score: 0.9669421487603306\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 2.215512273273319\n",
      "Out of Bag Accuracy Score: 0.9669421487603306\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 0.9398009570551125\n",
      "Out of Bag Accuracy Score: 0.9834710743801653\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 1.8167136700954245\n",
      "Out of Bag Accuracy Score: 0.9834710743801653\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 2.851694617331191\n",
      "Out of Bag Accuracy Score: 0.9826446280991735\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 0.7672588117156636\n",
      "Out of Bag Accuracy Score: 0.968595041322314\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.5643310980413645\n",
      "Out of Bag Accuracy Score: 0.9669421487603306\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 2.288100270242168\n",
      "Out of Bag Accuracy Score: 0.9677685950413223\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.9835\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.1857 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n",
      "CHIRPS time elapsed: 0.4566 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0304 seconds\n",
      "\n",
      "\n",
      "Running experiment for cardio_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.4797474614843367\n",
      "Out of Bag Accuracy Score: 0.9361559139784946\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 2.999992187795783\n",
      "Out of Bag Accuracy Score: 0.9354838709677419\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 4.393747712676742\n",
      "Out of Bag Accuracy Score: 0.9388440860215054\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.4258059941240333\n",
      "Out of Bag Accuracy Score: 0.926747311827957\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 2.833561589915746\n",
      "Out of Bag Accuracy Score: 0.926747311827957\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 4.272888406268265\n",
      "Out of Bag Accuracy Score: 0.9260752688172043\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.7868992940771875\n",
      "Out of Bag Accuracy Score: 0.9408602150537635\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 3.561484150539968\n",
      "Out of Bag Accuracy Score: 0.9401881720430108\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 5.3438370422273636\n",
      "Out of Bag Accuracy Score: 0.9428763440860215\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.5361861299722364\n",
      "Out of Bag Accuracy Score: 0.9294354838709677\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 3.1379884655809462\n",
      "Out of Bag Accuracy Score: 0.928763440860215\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 4.731326578380759\n",
      "Out of Bag Accuracy Score: 0.9294354838709677\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.9429\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.6750 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.8277 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0749 seconds\n",
      "\n",
      "\n",
      "Running experiment for credit_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 0.664097453041137\n",
      "Out of Bag Accuracy Score: 0.8737060041407867\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 1.368747457277749\n",
      "Out of Bag Accuracy Score: 0.8737060041407867\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 2.072724710542815\n",
      "Out of Bag Accuracy Score: 0.8799171842650103\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 0.6291805057861382\n",
      "Out of Bag Accuracy Score: 0.8757763975155279\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.2700195245011514\n",
      "Out of Bag Accuracy Score: 0.8757763975155279\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 1.9216369812921812\n",
      "Out of Bag Accuracy Score: 0.8757763975155279\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 0.727397941063515\n",
      "Out of Bag Accuracy Score: 0.8737060041407867\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 1.4924747440451256\n",
      "Out of Bag Accuracy Score: 0.8737060041407867\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 2.2427882838570383\n",
      "Out of Bag Accuracy Score: 0.8757763975155279\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 0.6648666546879554\n",
      "Out of Bag Accuracy Score: 0.8716356107660456\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.2945122870949035\n",
      "Out of Bag Accuracy Score: 0.8757763975155279\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 1.8893770848819997\n",
      "Out of Bag Accuracy Score: 0.8757763975155279\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.8799\n",
      "Best parameters: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.6141 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.3632 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0667 seconds\n",
      "\n",
      "\n",
      "Running experiment for german_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 0.8463240273955819\n",
      "Out of Bag Accuracy Score: 0.7485714285714286\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 1.679281373361448\n",
      "Out of Bag Accuracy Score: 0.7442857142857143\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 2.5716090676855288\n",
      "Out of Bag Accuracy Score: 0.7457142857142857\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 0.769951618418304\n",
      "Out of Bag Accuracy Score: 0.7285714285714285\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.550176585862431\n",
      "Out of Bag Accuracy Score: 0.7314285714285714\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 2.316769857871975\n",
      "Out of Bag Accuracy Score: 0.7342857142857143\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.1751700506531222\n",
      "Out of Bag Accuracy Score: 0.7514285714285714\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 2.333834115655492\n",
      "Out of Bag Accuracy Score: 0.7514285714285714\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 3.491991589260749\n",
      "Out of Bag Accuracy Score: 0.7557142857142857\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 0.8802965993475027\n",
      "Out of Bag Accuracy Score: 0.7357142857142858\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 1.7286091331878595\n",
      "Out of Bag Accuracy Score: 0.7428571428571429\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 2.571581424501346\n",
      "Out of Bag Accuracy Score: 0.74\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.7557\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.7539 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.7265 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0747 seconds\n",
      "\n",
      "\n",
      "Running experiment for lending_tiny_samp_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.6099684927794158\n",
      "Out of Bag Accuracy Score: 0.9158180583842498\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 3.3501282703839763\n",
      "Out of Bag Accuracy Score: 0.9137813985064495\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 4.830363093224236\n",
      "Out of Bag Accuracy Score: 0.9158180583842498\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.5401003447585708\n",
      "Out of Bag Accuracy Score: 0.9008825526137135\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 3.092328836107754\n",
      "Out of Bag Accuracy Score: 0.9063136456211812\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 4.767743168377621\n",
      "Out of Bag Accuracy Score: 0.9042769857433809\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 2.2990929429955713\n",
      "Out of Bag Accuracy Score: 0.9443312966734555\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 4.617402105569312\n",
      "Out of Bag Accuracy Score: 0.9456890699253224\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 7.071967527671717\n",
      "Out of Bag Accuracy Score: 0.9456890699253224\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.8202531995483184\n",
      "Out of Bag Accuracy Score: 0.9192124915139172\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 3.6058830104389017\n",
      "Out of Bag Accuracy Score: 0.9226069246435845\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 5.546642164389198\n",
      "Out of Bag Accuracy Score: 0.9192124915139172\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.9457\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.4697 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:387: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:391: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRPS time elapsed: 0.7559 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0865 seconds\n",
      "\n",
      "\n",
      "Running experiment for nursery_samp_data with random state = 124\n",
      "\n",
      "Split data into train-test and build RF\n",
      "New grid tuning... (please wait)\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.1386606156136736\n",
      "Out of Bag Accuracy Score: 0.9366041896361632\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 2.2939615266969895\n",
      "Out of Bag Accuracy Score: 0.9377067254685777\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 3.430476790842704\n",
      "Out of Bag Accuracy Score: 0.9371554575523704\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.0601698974137435\n",
      "Out of Bag Accuracy Score: 0.9250275633958104\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 2.037469735220384\n",
      "Out of Bag Accuracy Score: 0.9272326350606395\n",
      "\n",
      "Trying params: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 3.20422784473908\n",
      "Out of Bag Accuracy Score: 0.9288864388092613\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Training time: 1.7072085011204763\n",
      "Out of Bag Accuracy Score: 0.9586549062844543\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "Training time: 3.3701900108349605\n",
      "Out of Bag Accuracy Score: 0.960308710033076\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "Training time: 5.06392666623799\n",
      "Out of Bag Accuracy Score: 0.961962513781698\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Training time: 1.209286847913802\n",
      "Out of Bag Accuracy Score: 0.9277839029768468\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "Training time: 2.4668513151244724\n",
      "Out of Bag Accuracy Score: 0.9305402425578831\n",
      "\n",
      "Trying params: {'max_depth': 16, 'min_samples_leaf': 5, 'n_estimators': 1500}\n",
      "Training time: 3.7321062964563225\n",
      "Out of Bag Accuracy Score: 0.9299889746416758\n",
      "\n",
      "Best OOB Accuracy Estimate during tuning: 0.9620\n",
      "Best parameters: {'max_depth': 16, 'min_samples_leaf': 1, 'n_estimators': 1500}\n",
      "\n",
      "\n",
      "Prepare Unseen Data and Predictions\n",
      "\n",
      "Walking forest for 2 instances... (please wait)\n",
      "Forest Walk with async = False\n",
      "Forest Walk time elapsed: 0.6640 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 2 instances... (please wait)\n",
      "CHIRPS time elapsed: 0.6196 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "CHIRPS batch results eval time elapsed: 0.0785 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for random_state in [123, 124]:\n",
    "    for d_constructor in rp.datasets:\n",
    "        print('Running experiment for ' + d_constructor.__name__ + ' with random state = ' + str(random_state))\n",
    "        print()\n",
    "        # 1. Data and Forest prep\n",
    "        print('Split data into train-test and build RF') \n",
    "        rf, tt, meta_data = rp.data_forest_prep(d_constructor, project_dir=project_dir,\n",
    "                                      override_tuning=False,\n",
    "                                      random_state=random_state,\n",
    "                                      random_state_splits=random_state_splits)\n",
    "        \n",
    "        save_path = meta_data['get_save_path']()\n",
    "        print()\n",
    "\n",
    "        # 2. Prepare Unseen Data and Predictions\n",
    "        print('Prepare Unseen Data and Predictions')\n",
    "        # OPTION 1 - batching (to be implemented in the new code, right now it will do just one batch)\n",
    "        # this will normalise the above parameters to the size of the dataset\n",
    "        n_instances, n_batches = rt.batch_instance_ceiling(data_split=tt, n_instances=n_instances, batch_size=batch_size)\n",
    "\n",
    "        # this gets the next batch out of the data_split_container according to the required number of instances\n",
    "        # all formats can be extracted, depending on the requirement\n",
    "        # unencoded, encoded (sparse matrix is the type returned by scikit), ordinary dense matrix also available\n",
    "        instances, instances_enc, instances_enc_matrix, labels = tt.get_next(batch_size, which_split='test') # default\n",
    "\n",
    "        # OPTION 2 - just run with whole test set\n",
    "        # instances = tt.X_test; instances_enc = tt.X_test_enc; instances_enc_matrix = tt.X_test_enc_matrix; labels = tt.y_test\n",
    "\n",
    "        # Make all the model predictions from the decision forest\n",
    "        preds = rf.predict(X=instances_enc)\n",
    "        print()\n",
    "        \n",
    "        # 3.1 - Extract Tree Prediction Paths\n",
    "\n",
    "        print('Walking forest for ' + str(len(labels)) + ' instances... (please wait)')\n",
    "\n",
    "        # wrapper object needs the decision forest itself and the dataset meta data (we have a convenience function for this)\n",
    "        f_walker = strcts.forest_walker(forest = rf, meta_data=meta_data)\n",
    "\n",
    "        # set the timer\n",
    "        forest_walk_start_time = timeit.default_timer()\n",
    "\n",
    "        # do the walk - returns a batch_paths_container (even for just one instance)\n",
    "        # requires the X instances in a matrix (dense, ordinary numpy matrix) - this is available in the data_split_container\n",
    "        bp_container = f_walker.forest_walk(instances = instances_enc_matrix\n",
    "                                , labels = preds # we're explaining the prediction, not the true label!\n",
    "                                , forest_walk_async = forest_walk_async)\n",
    "\n",
    "        # stop the timer\n",
    "        forest_walk_end_time = timeit.default_timer()\n",
    "        forest_walk_elapsed_time = forest_walk_end_time - forest_walk_start_time\n",
    "\n",
    "        print('Forest Walk with async = ' + str(forest_walk_async))\n",
    "        print('Forest Walk time elapsed:', \"{:0.4f}\".format(forest_walk_elapsed_time), 'seconds')\n",
    "        print()\n",
    "        \n",
    "        # 3.2-3.4 - Freqent pattern mining of paths, Score and sort mined path segments, Merge path segments into one rule\n",
    "\n",
    "        # build CHIRPS and a rule for each instance represented in the batch paths container\n",
    "        CHIRPS = strcts.batch_CHIRPS_explainer(bp_container,\n",
    "                                        forest=rf,\n",
    "                                        sample_instances=tt.X_train_enc, # any representative sample can be used\n",
    "                                        sample_labels=tt.y_train,  # any representative sample can be used\n",
    "                                        meta_data=meta_data)\n",
    "\n",
    "        print('Running CHIRPS on a batch of ' + str(len(labels)) + ' instances... (please wait)')\n",
    "        # start a timer\n",
    "        ce_start_time = timeit.default_timer()\n",
    "\n",
    "        CHIRPS.batch_run_CHIRPS(chirps_explanation_async=chirps_explanation_async) # all the defaults\n",
    "\n",
    "        ce_end_time = timeit.default_timer()\n",
    "        ce_elapsed_time = ce_end_time - ce_start_time\n",
    "        print('CHIRPS time elapsed:', \"{:0.4f}\".format(ce_elapsed_time), 'seconds')\n",
    "        print('CHIRPS with async = ' + str(chirps_explanation_async))\n",
    "        print()\n",
    "        \n",
    "        # 4. Evaluating CHIRPS Explanations\n",
    "        print('Evaluating found explanations')\n",
    "        \n",
    "        results_start_time = timeit.default_timer()\n",
    "\n",
    "        # iterate over all the test instances (based on the ids in the index)\n",
    "        # scoring will leave out the specific instance by this id.\n",
    "        rt.evaluate_CHIRPS_explainers(CHIRPS, tt, labels.index,\n",
    "                                      print_to_screen=False, # set True when running single instances\n",
    "                                      save_results_path=save_path,\n",
    "                                      save_results_file='results' + '_rnst_' + str(random_state),\n",
    "                                      save_CHIRPS=True)\n",
    "\n",
    "        results_end_time = timeit.default_timer()\n",
    "        results_elapsed_time = results_end_time - results_start_time\n",
    "        print('CHIRPS batch results eval time elapsed:', \"{:0.4f}\".format(results_elapsed_time), 'seconds')\n",
    "        # this completes the CHIRPS runs\n",
    "\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
