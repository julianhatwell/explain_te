{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up notebook and load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# load what we need\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.reproducible as rp\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset can be turned into a container by invoking the constructor found in the module structures.py, the only mandatory parameters are an object that can be input to pandas.DataFrame and a class column name. Your object needs either its own column names, or these should be passed as a list to the var_names parameter.\n",
    "\n",
    "Several datasets are available as pre-prepared containers that hold the data and some meta-data that is used in the algorithm. If these pre-packaged sets have more than 10,000 rows then we've included some downsampled versions. You can see what there is by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the builtins, np, pd, urllib and cfg. The rest are dataset constructors that will load specific datasets.\n",
    "if False:\n",
    "    dir(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see more information about each dataset with something like this for the adult dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(ds.adult().spiel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of one dataset is given below. Note that the random_state propagates through other functions via the meta_data and is easily updated to allow alternative runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    cardio = ds.cardio(random_state=123, project_dir=project_dir)\n",
    "    meta_data = cardio.get_meta()\n",
    "    meta_data['random_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of datasets used in \"CHIRPS: Explaining Random Forest Classification\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    datasets = [\n",
    "            ds.adult,\n",
    "            ds.bankmark,\n",
    "            ds.car,\n",
    "            ds.cardio,\n",
    "            ds.credit,\n",
    "            ds.german,\n",
    "            ds.lending_tiny_samp,\n",
    "            ds.nursery,\n",
    "            ds.rcdv,\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of datasets used in \"Explaining Multi-class AdaBoost Classification in Medical Applications\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    datasets = [\n",
    "                ds.breast,\n",
    "                ds.cardio,\n",
    "                ds.diaretino,\n",
    "                ds.heart,\n",
    "                ds.mhtech14,\n",
    "                ds.mh2tech16,\n",
    "                ds.readmit,\n",
    "                ds.thyroid,\n",
    "                dsp.usoc2,\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising train-test splitting across environments\n",
    "To compare with methods in different environments, as we did in \"CHIRPS: Explaining Random Forest Classification,\" you can maintain the same dataset splits. The train test data is split with the one-time random_state_splits and the splits are saved to csv in the project folders. This way you can do same model with different data splits, or same data splits with different model. This is what's happening behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### do not run this here\n",
    "# train_index, test_index = mydata.get_tt_split_idx(random_state=random_state_splits)\n",
    "# tt = mydata.tt_split(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need here is the following, to take the same train_index, test_index numbers to an external .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Runs\n",
    "From here on, everything is automated behind the scenes to perform runs across several datasets and comparing different algorithms with CHIRPS. If you want to run CHIRPS and control the parameters and view results and performance directly, look at the CHIRPS Examples Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported train-test data for 1 datasets.\n",
      "Preprocessing cardio data and model for cardio with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "Train main model\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9409\n",
      "Best parameters:{'max_depth': 32.0, 'n_estimators': 600, 'oob_score': True, 'random_state': 123}\n",
      "\n",
      "Discretise data and train model, e.g. for Anchors\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.9409\n",
      "Best parameters:{'max_depth': 32.0, 'n_estimators': 600, 'oob_score': True, 'random_state': 123}\n",
      "\n",
      "\n",
      "Beginning benchmark for cardio data.\n",
      "Hyper-parameter settings: {'support_paths': 0.1, 'alpha_paths': 0.9, 'disc_path_bins': 4, 'score_func': 3, 'weighting': 'kldiv', 'merging_bootstraps': 20, 'pruning_bootstraps': 20, 'delta': 0.1}\n",
      "\n",
      "Prepare Unseen Data and Predictions for CHIRPS benchmark\n",
      "Walking forest for 10 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 1.3625 seconds\n",
      "\n",
      "Running CHIRPS on a batch of 10 instances... (please wait)\n",
      "Working on CHIRPS for instance 0 of 10\n",
      "as_chirps for batch_idx 0\n",
      "start mining for batch_idx 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 43 patterns from 592 for batch_idx 0\n",
      "start score sort for batch_idx 0 (43) patterns\n",
      "start merge rule for batch_idx 0 (43) patterns\n",
      "[('AC', False, 0.00174), ('MSTV', False, 0.58735)]\n",
      "indicative:\n",
      "0.9764216366158114 0.4607552335862187 0.10192131688733597 0.12312892322549493\n",
      "merge complete for batch_idx 0 (43) patterns\n",
      "start get explainer for batch_idx 0\n",
      "as_chirps for batch_idx 1\n",
      "start mining for batch_idx 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 39 patterns from 597 for batch_idx 1\n",
      "start score sort for batch_idx 1 (39) patterns\n",
      "start merge rule for batch_idx 1 (39) patterns\n",
      "[('AC', False, 0.00176), ('MSTV', False, 0.57726)]\n",
      "indicative:\n",
      "0.9764216366158114 0.4607552335862187 0.1382463626659789 0.1327568667344863\n",
      "merge complete for batch_idx 1 (39) patterns\n",
      "start get explainer for batch_idx 1\n",
      "as_chirps for batch_idx 2\n",
      "start mining for batch_idx 2\n",
      "found 12 patterns from 559 for batch_idx 2\n",
      "start score sort for batch_idx 2 (12) patterns\n",
      "start merge rule for batch_idx 2 (12) patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mean', True, 102.82)]\n",
      "indicative:\n",
      "0.9074074074074074 0.034132368003102015 0.40863566083040026 0.22249388753056235\n",
      "merge complete for batch_idx 2 (12) patterns\n",
      "start get explainer for batch_idx 2\n",
      "as_chirps for batch_idx 3\n",
      "start mining for batch_idx 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 49 patterns from 594 for batch_idx 3\n",
      "start score sort for batch_idx 3 (49) patterns\n",
      "start merge rule for batch_idx 3 (49) patterns\n",
      "[('AC', False, 0.00174)]\n",
      "indicative:\n",
      "0.976878612716763 0.44354897854413 0.11530677305348896 0.08281715989905943\n",
      "merge complete for batch_idx 3 (49) patterns\n",
      "start get explainer for batch_idx 3\n",
      "as_chirps for batch_idx 4\n",
      "start mining for batch_idx 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 47 patterns from 600 for batch_idx 4\n",
      "start score sort for batch_idx 4 (47) patterns\n",
      "start merge rule for batch_idx 4 (47) patterns\n",
      "[('AC', False, 0.00175)]\n",
      "indicative:\n",
      "0.976878612716763 0.44354897854413 0.11959767499257304 0.09363042988971414\n",
      "merge complete for batch_idx 4 (47) patterns\n",
      "start get explainer for batch_idx 4\n",
      "Working on CHIRPS for instance 5 of 10\n",
      "as_chirps for batch_idx 5\n",
      "start mining for batch_idx 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 39 patterns from 560 for batch_idx 5\n",
      "start score sort for batch_idx 5 (39) patterns\n",
      "start merge rule for batch_idx 5 (39) patterns\n",
      "[('AC', False, 0.00178), ('MSTV', False, 0.57037)]\n",
      "indicative:\n",
      "0.9763560500695411 0.4594717928241401 0.1503684339158464 0.15716272600834494\n",
      "merge complete for batch_idx 5 (39) patterns\n",
      "start get explainer for batch_idx 5\n",
      "as_chirps for batch_idx 6\n",
      "start mining for batch_idx 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 45 patterns from 494 for batch_idx 6\n",
      "start score sort for batch_idx 6 (45) patterns\n",
      "start merge rule for batch_idx 6 (45) patterns\n",
      "[('ASTV', True, 59.50549), ('Median', True, 150.63514)]\n",
      "indicative:\n",
      "0.9614921780986763 0.5060423576195637 0.08572942777888107 0.11485897063099737\n",
      "merge complete for batch_idx 6 (45) patterns\n",
      "start get explainer for batch_idx 6\n",
      "as_chirps for batch_idx 7\n",
      "start mining for batch_idx 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 42 patterns from 597 for batch_idx 7\n",
      "start score sort for batch_idx 7 (42) patterns\n",
      "start merge rule for batch_idx 7 (42) patterns\n",
      "[('AC', False, 0.00175)]\n",
      "indicative:\n",
      "0.976878612716763 0.44354897854413 0.13740259469989544 0.05701976685250887\n",
      "merge complete for batch_idx 7 (42) patterns\n",
      "start get explainer for batch_idx 7\n",
      "as_chirps for batch_idx 8\n",
      "start mining for batch_idx 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 53 patterns from 593 for batch_idx 8\n",
      "start score sort for batch_idx 8 (53) patterns\n",
      "start merge rule for batch_idx 8 (53) patterns\n",
      "[('AC', False, 0.00178), ('MSTV', False, 0.57959)]\n",
      "indicative:\n",
      "0.9763560500695411 0.4594717928241401 0.08032183721075875 0.10690067810131632\n",
      "merge complete for batch_idx 8 (53) patterns\n",
      "start get explainer for batch_idx 8\n",
      "as_chirps for batch_idx 9\n",
      "start mining for batch_idx 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1367: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5) # can result in nans\n",
      "C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py:1362: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5) # can result in nans if no value falls into bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 46 patterns from 566 for batch_idx 9\n",
      "start score sort for batch_idx 9 (46) patterns\n",
      "start merge rule for batch_idx 9 (46) patterns\n",
      "[('ALTV', True, 68.9819), ('ALTV', False, 7.18905), ('ASTV', False, 59.38818), ('ASTV', True, 79.51869), ('MSTV', True, 0.5411)]\n",
      "indicative:\n",
      "0.9354838709677419 0.03951329364330157 0.4712724238829725 0.44491625383345135\n",
      "merge complete for batch_idx 9 (46) patterns\n",
      "start get explainer for batch_idx 9\n",
      "CHIRPS time elapsed: 6.0048 seconds\n",
      "CHIRPS with async = False\n",
      "\n",
      "Evaluating found explanations\n",
      "Results saved to V:\\whiteboxing\\tests\\cardio\\RandomForest_CHIRPS_rnst_123\n",
      "CHIRPS batch results eval time elapsed: 2.5161 seconds\n",
      "\n",
      "Prepare Unseen Data and Predictions for Anchors benchmark\n",
      "Running Anchors on each instance and collecting results\n",
      "0: Working on Anchors for instance 63\n"
     ]
    }
   ],
   "source": [
    "# do the model tuning from scratch?\n",
    "override_tuning = False\n",
    "\n",
    "# Optional Memory and Computation Cost Management\n",
    "# CHIRPS is time economical but memory intensive to compute for lots of instances at once.\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=False\n",
    "n_cores = mp.cpu_count()-6\n",
    "\n",
    "# How many instances from the test set do you want to explain?\n",
    "# A number bigger than the test set will be interpreted as 'all'\n",
    "n_instances = 10\n",
    "start_instance = 0 # here can opt to start at a specific instance, diagnostic if something crashes\n",
    "\n",
    "# CHOOSE ONE\n",
    "model = 'RandomForest'\n",
    "# model = 'AdaBoost1' # SAMME\n",
    "# model = 'AdaBoost2' # SAMME.R\n",
    "# model = 'GBM'\n",
    "\n",
    "# CHOOSE ONE OR MORE\n",
    "do_CHIRPS = True\n",
    "do_Anchors = True\n",
    "do_dfrgTrs = False\n",
    "do_lore = False\n",
    "\n",
    "# list the dataset constructors you want to include\n",
    "datasets = [\n",
    "#         ds.adult,\n",
    "#         ds.bankmark,\n",
    "        ds.car,\n",
    "#         ds.cardio,\n",
    "#         ds.credit,\n",
    "#         ds.german,\n",
    "#         ds.lending_tiny_samp,\n",
    "#         ds.nursery,\n",
    "#        ds.rcdv,\n",
    "       ]\n",
    "# datasets = [ds.cervicalh] # here can opt for just one but it must be a list, e.g. datasets = [datasets[0]]\n",
    "\n",
    "# location to save results\n",
    "# project_dir = '/datadisk/whiteboxing/benchmarks'\n",
    "project_dir = 'V:\\\\whiteboxing\\\\tests' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "# project_dir = 'C:\\\\Users\\\\Crutt\\\\Documents\\\\whiteboxing\\\\tests'\n",
    "\n",
    "# set the random_state for various tasks. This is not the same as random.seed(), not system wide.\n",
    "random_state_splits = 123 # change this if you want to try different splits of the data into test / train\n",
    "random_state_rf = 123 # change this if you want to try with different forest construction\n",
    "random_state_exp = 123 # change this if you want to try with different runs of the explainer algorithm (affects bootstrap eval)\n",
    "\n",
    "# To standardise train-test splitting across environments - writes external files\n",
    "rp.export_data_splits(datasets=datasets, project_dir=project_dir, random_state_splits=random_state_splits)\n",
    "\n",
    "# How much messaging to print to the screen?\n",
    "verbose = True\n",
    "\n",
    "# CHIRPS default parameters - see papers for details\n",
    "merging_bootstraps = 20 # how many training bootstraps to test improvement in growing rule?\n",
    "pruning_bootstraps = 20 # how many training bootstraps to test deterioration in pruning rule?\n",
    "delta = 0.1 # pruning deterioration tolerance paramater\n",
    "\n",
    "# this is here if you want to pass parameters to the methods\n",
    "def benchmark_wrapper(do_CHIRPS=do_CHIRPS, do_Anchors=do_Anchors, do_dfrgTrs=do_dfrgTrs, do_lore=do_lore, **control):\n",
    "\n",
    "    if do_CHIRPS:\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "    \n",
    "    if do_Anchors:\n",
    "        control.update({'method' : 'Anchors'})\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "    if do_dfrgTrs:\n",
    "        control.update({'method' : 'defragTrees',\n",
    "                    'Kmax' : 10, 'restart' : 100, 'maxitr' : 20})\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "\n",
    "    if do_lore:\n",
    "        control.update({'method' : 'lore'})\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "\n",
    "# this is here to pass parameters to the model training and further parameters to CHIRPS\n",
    "tuning = {'override' : override_tuning}\n",
    "if model == 'RandomForest':\n",
    "    tuning.update({'grid' : {'n_estimators': [(i + 1) * 200 for i in range(8)],\n",
    "                            'max_depth' : [32]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=random_state_rf,\n",
    "                                           random_state_splits=random_state_splits,\n",
    "                                           do_raw=True, do_discretise=do_Anchors,\n",
    "                                           start_instance=start_instance,\n",
    "                                           verbose=verbose, n_cores=n_cores)\n",
    "\n",
    "    kwargs = {'support_paths' : 0.1, 'alpha_paths' : 0.9, 'disc_path_bins' : 4,\n",
    "             'score_func' : 3, 'weighting' : 'kldiv',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    " \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'chirps_explanation_async' : chirps_explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "    \n",
    "elif model == \"AdaBoost1\":\n",
    "    algo = 'SAMME'\n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                       random_state=random_state_rf,\n",
    "                                       random_state_splits=random_state_splits,\n",
    "                                       do_raw=True, do_discretise=do_Anchors,\n",
    "                                       start_instance=start_instance,\n",
    "                                       verbose=verbose, n_cores=n_cores)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "             'support_paths' : 0.1, 'alpha_paths' : 0.0,\n",
    "             'disc_path_bins' : 4, 'disc_path_eqcounts' : True,\n",
    "             'score_func' : 1, 'weighting' : 'kldiv',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    " \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'chirps_explanation_async' : chirps_explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "    \n",
    "elif model == 'AdaBoost2':\n",
    "    algo = 'SAMME.R'\n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                       random_state=random_state_rf,\n",
    "                                       random_state_splits=random_state_splits,\n",
    "                                       do_raw=True, do_discretise=do_Anchors,\n",
    "                                       start_instance=start_instance,\n",
    "                                       verbose=verbose, n_cores=n_cores)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "                 'support_paths' : 0.01, 'alpha_paths' : 0.0,\n",
    "                 'disc_path_bins' : 8, 'disc_path_eqcounts' : True,\n",
    "                 'score_func' : 1, 'weighting' : 'kldiv',\n",
    "                 'merging_bootstraps' : merging_bootstraps,\n",
    "                 'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    "    \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'chirps_explanation_async' : chirps_explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "    \n",
    "\n",
    "    \n",
    "else: # GBM - not fully implemented yet\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=123, random_state_splits=123,\n",
    "                                           start_instance=start_instance, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load what we need\n",
    "import numpy as np\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.reproducible as rp\n",
    "\n",
    "# prepare data\n",
    "datasets = [\n",
    "            ds.adult_small_samp,\n",
    "            ds.bankmark_samp,\n",
    "            ds.car,\n",
    "            ds.cardio,\n",
    "            ds.credit,\n",
    "            ds.german,\n",
    "            ds.lending_tiny_samp,\n",
    "            ds.nursery_samp,\n",
    "            ds.rcdv_samp\n",
    "           ]\n",
    "# datasets = [datasets[0]] # for testing can just choose one\n",
    "\n",
    "\n",
    "# CHIRPS default set up\n",
    "merging_bootstraps = 20\n",
    "pruning_bootstraps = 20\n",
    "delta = 0.1 # prune rule terms if loss of precision no greater than delta\n",
    "\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=True\n",
    "\n",
    "n_instances = 1000\n",
    "\n",
    "# model = 'RandomForest'\n",
    "# model = 'AdaBoost1'\n",
    "# model = 'AdaBoost2'\n",
    "# model = 'GBM'\n",
    "\n",
    "do_Anchors = False\n",
    "do_dfrgTrs = False\n",
    "\n",
    "# here can opt for just one, e.g. [datasets[0]] (as an iterator)\n",
    "start_instance = 0 # here can opt to start at a specific instance\n",
    "\n",
    "project_dir = 'V:\\\\whiteboxing\\\\tests' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "# project_dir = 'C:\\\\Users\\\\Crutt\\\\Documents\\\\whiteboxing\\\\tests'\n",
    "# project_dir = '/datadisk/whiteboxing'\n",
    "random_state_splits = 123 # change this if you want to try different splits of the data into test / train\n",
    "random_state_rf = 123 # change this if you want to try with different forest construction\n",
    "random_state_exp = 123 # change this if you want to try with different runs of the explainer algorithm (affects bootstrap eval)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "tuning = {'grid' : None, 'override' : False}\n",
    "\n",
    "# for troubleshooting, can isolate an instance and do like\n",
    "# bi_copy['cardio']['main']['ds_container'].current_row_test = 110\n",
    "\n",
    "if model == 'RandomForest':\n",
    "    tuning = {'grid' : {'n_estimators': [(i + 1) * 200 for i in range(8)], 'max_depth' : [32]},\n",
    "          'override' : override_tuning}\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=random_state_rf,\n",
    "                                           random_state_splits=random_state_splits,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "\n",
    "    alpha_paths = np.tile([0.9, 0.5, 0.1], 24)\n",
    "    disc_path_bins = np.tile(np.repeat([4, 8], 3), 12)\n",
    "    score_func = np.tile(np.repeat([5, 3, 1], 6), 4)\n",
    "    support_paths = np.tile(np.repeat([0.1, 0.05], 18), 2)\n",
    "    weighting = np.repeat(['chisq', 'nothing'], 36)\n",
    "\n",
    "    kwargs_grid = {k : {'alpha_paths' : ap, 'disc_path_bins' : dpb, 'disc_path_eqcounts' : False,\n",
    "                        'score_func' : sf, 'weighting' : w, 'support_paths' : sp,\n",
    "                        'merging_bootstraps' : merging_bootstraps,\n",
    "                        'pruning_bootstraps' : pruning_bootstraps,\n",
    "                        'which_trees' : 'majority',\n",
    "                        'delta' : delta} \n",
    "        for k, ap, dpb, sf, w, sp in zip(range(72), alpha_paths, disc_path_bins, score_func, weighting, support_paths)}\n",
    "    \n",
    "    \n",
    "    for kwargs in kwargs_grid:\n",
    "        bi_copy = rp.deepcopy(benchmark_items) # to avoid running down the internal counters\n",
    "        control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                    'n_instances' : n_instances,\n",
    "                    'random_state' : random_state_exp,\n",
    "                    'kwargs' : kwargs_grid[kwargs],\n",
    "                    'forest_walk_async' : forest_walk_async,\n",
    "                    'chirps_explanation_async' : chirps_explanation_async,\n",
    "                    'save_sensitivity_path' : 'rf_sensitivity'}\n",
    "\n",
    "        rp.do_benchmarking(bi_copy, verbose, **control)\n",
    "        \n",
    "elif model in ('AdaBoost1', 'AdaBoost2'):\n",
    "    if model == 'AdaBoost1':\n",
    "        algo = 'SAMME'\n",
    "        save_sensitivity_path = 'ada1_sensitivity'\n",
    "        support_paths = np.tile(np.tile([0.05, 0.02, 0.01], 16), 2)\n",
    "        n_kwargs = 36\n",
    "    else:\n",
    "        algo = 'SAMME.R'\n",
    "        save_sensitivity_path = 'ada2_sensitivity'\n",
    "        support_paths = np.tile(np.tile([0.005, 0.002, 0.001], 16), 2)\n",
    "        n_kwargs = 72\n",
    "        \n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=123, random_state_splits=123,\n",
    "                                           start_instance=start_instance, verbose=verbose)\n",
    "    \n",
    "    disc_path_bins = np.tile(np.tile(np.repeat([4, 8], 3), 6), 2)\n",
    "    disc_path_eqcounts = np.tile(np.tile(np.repeat([True, False], 6), 6), 2)\n",
    "    weighting = np.tile(np.repeat(['chisq', 'kldiv', 'nothing'], 12), 2)\n",
    "    which_trees = np.repeat(['majority', 'conf_weighted'], 36)\n",
    "\n",
    "    kwargs_grid = {k : {'paths_lengths_threshold' : 5, 'alpha_paths' : 0.0,\n",
    "                        'disc_path_bins' : dpb, 'disc_path_eqcounts' : dpeq,\n",
    "                        'score_func' : 1, 'weighting' : w, 'support_paths' : sp,\n",
    "                        'merging_bootstraps' : merging_bootstraps,\n",
    "                        'pruning_bootstraps' : pruning_bootstraps,\n",
    "                        'which_trees' : wchtr,\n",
    "                        'delta' : delta} \n",
    "    for k, dpb, dpeq, w, sp, wchtr \\\n",
    "                   in zip(range(n_kwargs), disc_path_bins, disc_path_eqcounts, weighting, support_paths, which_trees)}\n",
    "\n",
    "    for kwargs in kwargs_grid: # range(24, 36): #\n",
    "        bi_copy = rp.deepcopy(benchmark_items) # to avoid running down the internal counters\n",
    "        control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                    'n_instances' : n_instances,\n",
    "                    'random_state' : random_state_exp,\n",
    "                    'kwargs' : kwargs_grid[kwargs],\n",
    "                    'forest_walk_async' : forest_walk_async,\n",
    "                    'chirps_explanation_async' : chirps_explanation_async,\n",
    "                    'save_sensitivity_path' : save_sensitivity_path}\n",
    " \n",
    "        rp.do_benchmarking(bi_copy, verbose, **control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
