{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up notebook and load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (structures.py, line 973)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\id126493\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2910\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-6b468e04810b>\"\u001b[1;36m, line \u001b[1;32m7\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import CHIRPS.structures as strcts\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\id126493\\Documents\\GitHub\\explain_te\\CHIRPS\\structures.py\"\u001b[1;36m, line \u001b[1;32m973\u001b[0m\n\u001b[1;33m    self.features = meta_data.['features']\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# for notebook plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# load what we need\n",
    "import time\n",
    "import timeit\n",
    "import CHIRPS.structures as strcts\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.routines as rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common config - can be ommitted if defaults are OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'V:\\\\whiteboxing' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "random_state_splits = 123 # one off for splitting the data into test / train\n",
    "random_state = 123 # for everything else - e.g. building a new rf with same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a wrapper for the dataset. \n",
    "#### Use one that ships with the package, or create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one of the included datasets\n",
    "# project_dir will default to directory name CHIRPS in the working directory if not given\n",
    "# random_state will default to 123\n",
    "mydata = ds.cardio_data(random_state=random_state, project_dir=project_dir)\n",
    "meta_data = mydata.get_meta()\n",
    "\n",
    "# split the data. here using a basic sampling method.\n",
    "# the returned object is a wrapper class that contains\n",
    "# the train and test splits for X and y\n",
    "\n",
    "# also the the encoded versions of X_train and X_test that the rf will use\n",
    "# this is because we prefer onehot encoded over allowing categorical vars to be represented as integer\n",
    "# scikit would treat these as ordinal, which is inappropriate\n",
    "\n",
    "# also some meta-data: priors for y, the indexes from the input data\n",
    "\n",
    "# also some convenience functions for leave-one-out testing\n",
    "\n",
    "# train test split - one off hard-coded random state.\n",
    "# random state can be ommitted \n",
    "# and will default to the state held in the dataset container\n",
    "# which defaults to 123 if ommitted in the constructor\n",
    "train_index, test_index = mydata.get_tt_split_idx(random_state=random_state_splits)\n",
    "# optionally, indexes can be ommitted and will default to scikit's train_test_split method\n",
    "tt = mydata.tt_split(train_index, test_index)\n",
    "\n",
    "# build a random forest, tuned for high accuracy\n",
    "\n",
    "####################### PARAMETER TUNING #######################\n",
    "################### Only runs when required ####################\n",
    "### Use the override_tuning = True to force a new tuning run ###\n",
    "################################################################\n",
    "\n",
    "best_params = rt.tune_rf(\n",
    " X=tt.X_train_enc,\n",
    " y=tt.y_train,\n",
    " save_path = mydata.make_save_path(), # override_tuning=False, # default\n",
    " random_state=mydata.random_state)\n",
    "\n",
    "#### best_params are save to json at the save_path location ####\n",
    "################################################################\n",
    "\n",
    "# train a rf model. use onehot encoded training data\n",
    "# so that cat vars are not converted to ordinal\n",
    "# this is recommended practice with scikit-learn\n",
    "# the data_split_container is a convenience wrapper with everything you need\n",
    "rf = rt.train_rf(\n",
    " X=tt.X_train_enc,\n",
    " y=tt.y_train,\n",
    " best_params=best_params,\n",
    " random_state=mydata.random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the model and plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outputs of this function are:\n",
    "# cm - confusion matrix as 2d array\n",
    "# acc - accuracy of model = correctly classified instance / total number instances\n",
    "# coka - Cohen's kappa score. Accuracy adjusted for probability of correct by random guess. Useful for multiclass problems\n",
    "# prfs - precision, recall, f-score, support with the following signatures as a 2d array\n",
    "# 0 <= p, r, f <= 1. s = number of instances for each true class label (row sums of cm)\n",
    "cm, acc, coka, prfs = rt.evaluate_model(prediction_model=rf, X=tt.X_test_enc, y=tt.y_test,\n",
    "             class_names=mydata.get_label(mydata.class_col, [i for i in range(len(mydata.class_names))]).tolist(),\n",
    "             plot_cm=True, plot_cm_norm=True) # False here will output the metrics and suppress the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing unseen data\n",
    "\n",
    "Again note:\n",
    "test set has never been \"seen\" by random forest during training\n",
    "test set has been only used to assess model (random forest) accuracy - no additional tuning after this\n",
    "test set has not be involved in generating the explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional: memory and computation cost management\n",
    "#### CHIRPS is time economical but memory intensive to compute for lots of instances at once\n",
    "option 1: choose a smaller number of instances to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control for async processes - each tree walk can be done in its own core\n",
    "# and so can each explanation (e.g. rule conditions merge by hill-climbing)\n",
    "# these will default to false if not passed explicitly to the explainer function\n",
    "# on a multi-core machine there should be a good speed up for large batches\n",
    "# when the batch_size advantage exceeds the overhead of setting up multi-processing\n",
    "# timings will be printed to screen so you can see if it helps\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=False\n",
    "\n",
    "# the number of instances can be controlled by\n",
    "# batch_size - how many instances to explain at one time\n",
    "batch_size = 5\n",
    "# how many instances to explain in total from a test/unseen set\n",
    "n_instances = 10000 \n",
    "\n",
    "# this will normalise the above parameters to the size of the dataset\n",
    "rt.batch_instance_ceiling(data_split=tt, n_instances=n_instances, batch_size=batch_size)\n",
    "\n",
    "# this gets the next batch out of the data_split_container according to the required number of instances\n",
    "# all formats can be extracted, depending on the requirement\n",
    "# unencoded, encoded (sparse matrix is the type returned by scikit), ordinary dense matrix also available\n",
    "instances, instances_enc, instances_enc_matrix, labels = tt.get_next(batch_size, which_split='test') # default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 2: just run the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances = tt.X_test; instances_enc = tt.X_test_enc; instances_enc_matrix = tt.X_test_enc_matrix; labels = tt.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all the model predictions from the decision forest\n",
    "Important point, no compromise on model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the model predictions for the instances we're looking at\n",
    "pred_indices = labels.index\n",
    "pred_labels = rf.predict(X=instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHIRPS Step 1:\n",
    "## Extract Tree Prediction Paths\n",
    "### Fit a forest_walker object to the dataset and decision forest\n",
    "This is a wrapper will extracts the paths of all the given instances. For CHIRPS, we want a large sample. The whole training set or other representative sample will do.\n",
    "\n",
    "It can also report interesting statistics (treating the forest as a set of random tree-structured variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper object needs the decision forest itself and the dataset meta data (we have a convenience function for this)\n",
    "f_walker = strcts.forest_walker(forest = rf, meta_data=meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the work of extracting all the paths for each instance is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Walking forest for ' + str(len(labels)) + ' instances... (please wait)')\n",
    "\n",
    "# set the timer\n",
    "forest_walk_start_time = timeit.default_timer()\n",
    "\n",
    "# do the walk - returns a batch_paths_container (even for just one instance)\n",
    "# requires the X instances in a matrix (dense, ordinary numpy matrix) - this is available in the data_split_container\n",
    "bp_container = f_walker.forest_walk(instances = instances_enc_matrix\n",
    "                        , labels = pred_labels\n",
    "                        , forest_walk_async = forest_walk_async)\n",
    "\n",
    "# stop the timer\n",
    "forest_walk_end_time = timeit.default_timer()\n",
    "forest_walk_elapsed_time = forest_walk_end_time - forest_walk_start_time\n",
    "\n",
    "print('Forest Walk with async = ' + str(forest_walk_async))\n",
    "print('Forest Walk time elapsed:', \"{:0.4f}\".format(forest_walk_elapsed_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHIRPS Steps 2-4: \n",
    "## Freqent pattern mining of paths.\n",
    "## Score and sort mined path segments.\n",
    "## Merge path segments into one rule\n",
    "\n",
    "This is a wrapper object that will execute steps 2-4 on all the instance-paths in the batch_paths_container.\n",
    "\n",
    "Note that true_divide warnings are OK. It just means that a continuous variable is unbounded in some way i.e. no greater/less than discontinuity is used in the CHIRPS explanation.\n",
    "\n",
    "Note also, here we are using the training set to create the explainers. We could use a different dataset as long as it is representative of the training set that built the decision forest. Most important that we don't use the dataset that we wish to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CHIRPS and a rule for each instance represented in the batch paths container\n",
    "CHIRPS = strcts.batch_CHIRPS_explainer(bp_container,\n",
    "                                forest=rf,\n",
    "                                sample_instances=tt.X_train_enc, # any representative sample can be used\n",
    "                                sample_labels=tt.y_train,  # any representative sample can be used\n",
    "                                meta_data=meta_data)\n",
    "\n",
    "print('Running CHIRPS on a batch of ' + str(len(labels)) + ' instances... (please wait)')\n",
    "# start a timer\n",
    "ce_start_time = timeit.default_timer()\n",
    "\n",
    "CHIRPS.batch_run_CHIRPS(chirps_explanation_async=chirps_explanation_async) # all the defaults\n",
    "\n",
    "ce_end_time = timeit.default_timer()\n",
    "ce_elapsed_time = ce_end_time - ce_start_time\n",
    "print('CHIRPS time elapsed:', \"{:0.4f}\".format(ce_elapsed_time), 'seconds')\n",
    "print('CHIRPS with async = ' + str(chirps_explanation_async))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set has been used to create an explainer *one instance at a time*\n",
    "# rest of test set was not \"seen\" during construction of each explainer\n",
    "\n",
    "# to score each explainer, we use test set, leaving out the individual instance being explained\n",
    "# the data_split_container has a convenience funtion for this\n",
    "\n",
    "\n",
    "\n",
    "# iterate over all the test instances to determine the various scores using leave-one-out testing\n",
    "print('evaluating found explanations')\n",
    "results_start_time = timeit.default_timer()\n",
    "\n",
    "headers = ['instance_id', 'algorithm',\n",
    "                'pretty rule', 'rule length',\n",
    "                'pred class', 'pred class label',\n",
    "                'target class', 'target class label',\n",
    "                'majority voting trees', 'majority vote share', 'pred prior',\n",
    "                'precision(tr)', 'recall(tr)', 'f1(tr)',\n",
    "                'accuracy(tr)', 'lift(tr)',\n",
    "                'total coverage(tr)',\n",
    "                'precision(tt)', 'recall(tt)', 'f1(tt)',\n",
    "                'accuracy(tt)', 'lift(tt)',\n",
    "                'total coverage(tt)', 'model_acc', 'model_ck']\n",
    "output = [[]] * len(CHIRPS.CHIRPS_explainers)\n",
    "\n",
    "for i, c in enumerate(CHIRPS.CHIRPS_explainers):\n",
    "    \n",
    "    # instance meta data\n",
    "    alg = c.algorithm\n",
    "    instance_id = pred_indices[i]\n",
    "    mc = c.major_class\n",
    "    mc_lab = c.major_class_label\n",
    "    tc = c.target_class\n",
    "    tc_lab = c.target_class_label\n",
    "    vt = c.model_votes['counts'][tc]\n",
    "    mvs = c.model_posterior[tc]\n",
    "    prior = c.posterior[0][tc]\n",
    "    rule = c.pruned_rule\n",
    "    pretty_rule = c.prettify_rule()\n",
    "    rule_len = len(rule)\n",
    "    # final metrics from rule merge step (usually based on training set)\n",
    "    tr_prec = list(reversed(c.posterior))[tc]\n",
    "    tr_recall = list(reversed(c.recall))[tc]\n",
    "    tr_f1 = list(reversed(c.f1))[tc]\n",
    "    tr_acc = list(reversed(c.accuracy))[tc]\n",
    "    tr_lift = list(reversed(c.lift))[tc]\n",
    "    tr_coverage = list(reversed(c.coverage))\n",
    "    \n",
    "    # get test sample by leave-one-out on current instance\n",
    "    instances, instances_enc, labels = tt.get_loo_instances(instance_id)\n",
    "    # then evaluating rule metrics on test set\n",
    "    eval_rule = c.evaluate_rule(rule=c.pruned_rule, instances=instances_enc, labels=labels)\n",
    "    \n",
    "    # collect results\n",
    "    tt_prec = eval_rule['post'][tc]\n",
    "    tt_recall = eval_rule['recall'][tc]\n",
    "    tt_f1 = eval_rule['f1'][tc]\n",
    "    tt_acc = eval_rule['accuracy'][tc]\n",
    "    tt_lift = eval_rule['lift'][tc]\n",
    "    tt_coverage = eval_rule['coverage']\n",
    "    \n",
    "    output[i] = [instance_id,\n",
    "        alg,\n",
    "        pretty_rule,\n",
    "        rule_len,\n",
    "        mc,\n",
    "        mc_lab,\n",
    "        tc,\n",
    "        tc_lab,\n",
    "        vt,\n",
    "        mvs,\n",
    "        prior,\n",
    "        tr_prec,\n",
    "        tr_recall,\n",
    "        tr_f1,\n",
    "        tr_acc,\n",
    "        tr_lift,\n",
    "        tr_coverage,\n",
    "        tt_prec,\n",
    "        tt_recall,\n",
    "        tt_f1,\n",
    "        tt_acc,\n",
    "        tt_lift,\n",
    "        tt_coverage,\n",
    "        acc,\n",
    "        coka]\n",
    "    print('instance id: ' + str(instance_id) + ' with target class ' + str(tc) + ' (' + tc_lab + ')')\n",
    "    print('rule: ' + pretty_rule)\n",
    "    print()\n",
    "    print('rule precision (unseen data) ' + str(tt_prec))\n",
    "    print('rule coverage (unseen data) ' + str(tt_coverage))\n",
    "    print('rule f1 score (unseen data) ' + str(tt_f1))\n",
    "    print('rule lift (unseen data) ' + str(tt_lift))\n",
    "    print('unseen data prior counts ' + str(eval_rule['priors']['counts']))\n",
    "    print('rule posterior counts ' + str(eval_rule['counts']))\n",
    "    print('rule chisq p-value ' + str(eval_rule['chisq'][1]))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "results_end_time = timeit.default_timer()\n",
    "results_elapsed_time = results_end_time - results_start_time\n",
    "print('CHIRPS batch results eval time elapsed:', \"{:0.4f}\".format(results_elapsed_time), 'seconds')\n",
    "# this completes the CHIRPS runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
