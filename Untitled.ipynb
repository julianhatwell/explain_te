{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using previous tuning parameters\n"
     ]
    }
   ],
   "source": [
    "# for notebook plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# load what we need\n",
    "import numpy as np\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.reproducible as rp\n",
    "import CHIRPS.structures as strcts\n",
    "from CHIRPS.routines import extend_path\n",
    "from lime import lime_tabular as limtab\n",
    "from anchor import anchor_tabular as anchtab\n",
    "\n",
    "project_dir = 'V:\\\\whiteboxing' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "random_state_splits = 123 # one off for splitting the data into test / train\n",
    "random_state=123\n",
    "\n",
    "mydata = rp.datasets[8](random_state=random_state, project_dir=project_dir)\n",
    "meta_data = mydata.get_meta()\n",
    "save_path = meta_data['get_save_path']()\n",
    "train_index, test_index = mydata.get_tt_split_idx(random_state=123)\n",
    "tt = mydata.tt_split(train_index, test_index)\n",
    "\n",
    "# new copy of ds_container (need to reset the row counters)\n",
    "tt_anch = mydata.tt_split(train_index, test_index)\n",
    "# preprocessing - discretised continuous X matrix has been added and also needs an updated var_dict \n",
    "# plus returning the fitted explainer that holds the data distribution\n",
    "tt_anch, anchors_explainer = rp.Anchors_preproc(ds_container=tt_anch,\n",
    "                                                 meta_data=meta_data)\n",
    "\n",
    "# re-fitting the random forest to the discretised data and evaluating\n",
    "rf = rp.forest_prep(ds_container=tt_anch,\n",
    "                meta_data=meta_data,\n",
    "                save_path=save_path,\n",
    "                identifier='Anchors')\n",
    "\n",
    "# rp.Anchors_benchmark(forest=rf, ds_container=tt_anch, meta_data=meta_data,\n",
    "#                     anchors_explainer=anchors_explainer,\n",
    "#                     batch_size=batch_size, n_instances=n_instances,\n",
    "#                     save_path=save_path, dataset_name=d_constructor.__name__,\n",
    "#                     random_state=meta_data['random_state'])\n",
    "\n",
    "\n",
    "    # OPTION 1 - batching (to be implemented in the new code, right now it will do just one batch)\n",
    "_, instances_matrix, instances_enc, _, labels = rp.unseen_data_prep(tt_anch,\n",
    "                                        n_instances=10000,\n",
    "                                        batch_size=10000)\n",
    "\n",
    "preds = rf.predict(instances_enc)\n",
    "sample_labels = rf.predict(tt_anch.X_train_enc) # for train estimates\n",
    "\n",
    "# start the explanation process\n",
    "explanation = rp.Anchors_explanation(instances_matrix[23], anchors_explainer, rf,\n",
    "                                    threshold=0.95,\n",
    "                                    random_state=random_state)\n",
    "\n",
    "anchor_train_idx = np.array([all_eq.all() for all_eq in tt_anch.X_train_matrix[:, explanation.features()] == instances_matrix[:,explanation.features()][23]])\n",
    "\n",
    "evaluator = strcts.evaluator()\n",
    "train_metrics = evaluator.evaluate(prior_labels=sample_labels, post_idx=anchor_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23], dtype=int64),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(labels.index == 680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1980,    1,    1,    0,    1,    0,    0,    0,    0,    0,\n",
       "            1,    1,    9,    1,  256,   26,   55,    0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_matrix[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.0,\n",
       " 'xcoverage': 0.0,\n",
       " 'stability': array([0., 0.]),\n",
       " 'prior': {'labels': [0, 1],\n",
       "  'counts': array([833, 489], dtype=int64),\n",
       "  'p_counts': array([0.6301059, 0.3698941]),\n",
       "  's_counts': array([0.62962963, 0.36961451])},\n",
       " 'posterior': array([0., 0.]),\n",
       " 'counts': array([0, 0], dtype=int64),\n",
       " 'labels': [0, 1],\n",
       " 'recall': array([0., 0.]),\n",
       " 'f1': array([0., 0.]),\n",
       " 'accuracy': array([0.3698941, 0.6301059]),\n",
       " 'lift': array([0., 0.]),\n",
       " 'chisq': nan,\n",
       " 'kl_div': 0.7910309642879136}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
