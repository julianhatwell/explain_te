{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up notebook and load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load what we need\n",
    "import numpy as np\n",
    "import CHIRPS.datasets as ds\n",
    "import CHIRPS.datasets_proprietary as dsp\n",
    "import CHIRPS.reproducible as rp\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset can be turned into a container by invoking the constructor found in the module structures.py, the only mandatory parameters are an object that can be input to pandas.DataFrame and a class column name. Your object needs either its own column names, or these should be passed as a list to the var_names parameter.\n",
    "\n",
    "Several datasets are available as pre-prepared containers that hold the data and some meta-data that is used in the algorithm. If these pre-packaged sets have more than 10,000 rows then we've included some downsampled versions. You can see what there is by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the builtins, np, pd, urllib and cfg. The rest are dataset constructors that will load specific datasets.\n",
    "if False:\n",
    "    dir(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see more information about each dataset with something like this for the adult dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(ds.adult().spiel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of one dataset is given below. Note that the random_state propagates through other functions via the meta_data and is easily updated to allow alternative runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    cardio = ds.cardio(random_state=123, project_dir=project_dir)\n",
    "    meta_data = cardio.get_meta()\n",
    "    meta_data['random_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of datasets used in \"CHIRPS: Explaining Random Forest Classification\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    datasets = [\n",
    "            ds.adult,\n",
    "            ds.bankmark,\n",
    "            ds.car,\n",
    "            ds.cardio,\n",
    "            ds.credit,\n",
    "            ds.german,\n",
    "            ds.lending_tiny_samp,\n",
    "            ds.nursery,\n",
    "            ds.rcdv,\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of datasets used in \"Explaining Multi-class AdaBoost Classification in Medical Applications\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    datasets = [\n",
    "                ds.breast,\n",
    "                ds.cardio,\n",
    "                ds.diaretino,\n",
    "                ds.heart,\n",
    "                ds.mhtech14,\n",
    "                ds.mh2tech16,\n",
    "                ds.readmit,\n",
    "                ds.thyroid,\n",
    "                dsp.usoc2,\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising train-test splitting across environments\n",
    "To compare with methods in different environments, as we did in \"CHIRPS: Explaining Random Forest Classification,\" you can maintain the same dataset splits. The train test data is split with the one-time random_state_splits and the splits are saved to csv in the project folders. This way you can do same model with different data splits, or same data splits with different model. This is what's happening behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### do not run this here\n",
    "# train_index, test_index = mydata.get_tt_split_idx(random_state=random_state_splits)\n",
    "# tt = mydata.tt_split(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need here is the following, to take the same train_index, test_index numbers to an external .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Runs\n",
    "From here on, everything is automated behind the scenes to perform runs across several datasets and comparing different algorithms with CHIRPS. If you want to run CHIRPS and control the parameters and view results and performance directly, look at the CHIRPS Examples Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julianhatwell/anaconda3/envs/B3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported train-test data for 2 datasets.\n",
      "Preprocessing adult data and model for adult with random state = 123\n",
      "Split data into main train-test and build forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julianhatwell/anaconda3/envs/B3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train main model\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.8707\n",
      "Best parameters:{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1600, 'subsample': 0.5, 'random_state': 123}\n",
      "\n",
      "\n",
      "Preprocessing rcdv data and model for rcdv with random state = 123\n",
      "Split data into main train-test and build forest\n",
      "Train main model\n",
      "using previous tuning parameters\n",
      "Best OOB Accuracy Estimate during tuning: 0.6802\n",
      "Best parameters:{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5, 'random_state': 123}\n",
      "\n",
      "\n",
      "Running defragTrees\n",
      "\n",
      "[Seed 123] TrainingError = 0.16, K = 5\n",
      "[Seed 124] TrainingError = 0.17, K = 6\n",
      "[Seed 125] TrainingError = 0.24, K = 2\n",
      "[Seed 126] TrainingError = 0.24, K = 4\n",
      "[Seed 127] TrainingError = 0.24, K = 3\n",
      "[Seed 128] TrainingError = 0.17, K = 6\n",
      "[Seed 129] TrainingError = 0.22, K = 5\n",
      "[Seed 130] TrainingError = 0.16, K = 6\n",
      "[Seed 131] TrainingError = 0.18, K = 4\n",
      "[Seed 132] TrainingError = 0.17, K = 5\n",
      "Optimal Model >> Seed 130, TrainingError = 0.16, K = 6\n",
      "Fit defragTrees time elapsed: 6607.4162 seconds\n",
      "\n",
      "defragTrees test accuracy\n",
      "Accuracy = 0.843309\n",
      "Coverage = 0.998908\n",
      "Overlap = 0.175118\n",
      "defragTrees benchmark\n",
      "0: Working on defragTrees for instance 20668\n",
      "10: Working on defragTrees for instance 41751\n",
      "20: Working on defragTrees for instance 48836\n",
      "30: Working on defragTrees for instance 30926\n",
      "40: Working on defragTrees for instance 34112\n",
      "50: Working on defragTrees for instance 22699\n",
      "60: Working on defragTrees for instance 8756\n",
      "70: Working on defragTrees for instance 31547\n",
      "80: Working on defragTrees for instance 13216\n",
      "90: Working on defragTrees for instance 9043\n",
      "100: Working on defragTrees for instance 46391\n",
      "110: Working on defragTrees for instance 16960\n",
      "120: Working on defragTrees for instance 2803\n",
      "130: Working on defragTrees for instance 9310\n",
      "140: Working on defragTrees for instance 37175\n",
      "150: Working on defragTrees for instance 24581\n",
      "160: Working on defragTrees for instance 8895\n",
      "170: Working on defragTrees for instance 10932\n",
      "180: Working on defragTrees for instance 11051\n",
      "190: Working on defragTrees for instance 4661\n",
      "200: Working on defragTrees for instance 30762\n",
      "210: Working on defragTrees for instance 34883\n",
      "220: Working on defragTrees for instance 38409\n",
      "230: Working on defragTrees for instance 15444\n",
      "240: Working on defragTrees for instance 13996\n",
      "250: Working on defragTrees for instance 17834\n",
      "260: Working on defragTrees for instance 17626\n",
      "270: Working on defragTrees for instance 20709\n",
      "280: Working on defragTrees for instance 45990\n",
      "290: Working on defragTrees for instance 47520\n",
      "300: Working on defragTrees for instance 14038\n",
      "310: Working on defragTrees for instance 44937\n",
      "320: Working on defragTrees for instance 27862\n",
      "330: Working on defragTrees for instance 21204\n",
      "340: Working on defragTrees for instance 37048\n",
      "350: Working on defragTrees for instance 34727\n",
      "360: Working on defragTrees for instance 39759\n",
      "370: Working on defragTrees for instance 13334\n",
      "380: Working on defragTrees for instance 42156\n",
      "390: Working on defragTrees for instance 39051\n",
      "400: Working on defragTrees for instance 28180\n",
      "410: Working on defragTrees for instance 15807\n",
      "420: Working on defragTrees for instance 42920\n",
      "430: Working on defragTrees for instance 37097\n",
      "440: Working on defragTrees for instance 4155\n",
      "450: Working on defragTrees for instance 48300\n",
      "460: Working on defragTrees for instance 8328\n",
      "470: Working on defragTrees for instance 8363\n",
      "480: Working on defragTrees for instance 43635\n",
      "490: Working on defragTrees for instance 26055\n",
      "500: Working on defragTrees for instance 4951\n",
      "510: Working on defragTrees for instance 2197\n",
      "520: Working on defragTrees for instance 3152\n",
      "530: Working on defragTrees for instance 40020\n",
      "540: Working on defragTrees for instance 30718\n",
      "550: Working on defragTrees for instance 25987\n",
      "560: Working on defragTrees for instance 26942\n",
      "570: Working on defragTrees for instance 39153\n",
      "580: Working on defragTrees for instance 33944\n",
      "590: Working on defragTrees for instance 118\n",
      "600: Working on defragTrees for instance 15306\n",
      "610: Working on defragTrees for instance 16952\n",
      "620: Working on defragTrees for instance 16336\n",
      "630: Working on defragTrees for instance 2308\n",
      "640: Working on defragTrees for instance 32504\n",
      "650: Working on defragTrees for instance 20284\n",
      "660: Working on defragTrees for instance 37669\n",
      "670: Working on defragTrees for instance 12106\n",
      "680: Working on defragTrees for instance 27294\n",
      "690: Working on defragTrees for instance 1\n",
      "700: Working on defragTrees for instance 37519\n",
      "710: Working on defragTrees for instance 39259\n",
      "720: Working on defragTrees for instance 38694\n",
      "730: Working on defragTrees for instance 26448\n",
      "740: Working on defragTrees for instance 22871\n",
      "750: Working on defragTrees for instance 20856\n",
      "760: Working on defragTrees for instance 15566\n",
      "770: Working on defragTrees for instance 6441\n",
      "780: Working on defragTrees for instance 34211\n",
      "790: Working on defragTrees for instance 41293\n",
      "800: Working on defragTrees for instance 8436\n",
      "810: Working on defragTrees for instance 13029\n",
      "820: Working on defragTrees for instance 25953\n",
      "830: Working on defragTrees for instance 31820\n",
      "840: Working on defragTrees for instance 11352\n",
      "850: Working on defragTrees for instance 44622\n",
      "860: Working on defragTrees for instance 16015\n",
      "870: Working on defragTrees for instance 47206\n",
      "880: Working on defragTrees for instance 18648\n",
      "890: Working on defragTrees for instance 28964\n",
      "900: Working on defragTrees for instance 45660\n",
      "910: Working on defragTrees for instance 23407\n",
      "920: Working on defragTrees for instance 47813\n",
      "930: Working on defragTrees for instance 44517\n",
      "940: Working on defragTrees for instance 24908\n",
      "950: Working on defragTrees for instance 43063\n",
      "960: Working on defragTrees for instance 40343\n",
      "970: Working on defragTrees for instance 756\n",
      "980: Working on defragTrees for instance 6517\n",
      "990: Working on defragTrees for instance 9857\n",
      "Running defragTrees\n",
      "\n",
      "[Seed 123] TrainingError = 0.37, K = 6\n",
      "[Seed 124] TrainingError = 0.36, K = 8\n",
      "[Seed 125] TrainingError = 0.38, K = 4\n",
      "[Seed 126] TrainingError = 0.38, K = 6\n",
      "[Seed 127] TrainingError = 0.37, K = 7\n",
      "[Seed 128] TrainingError = 0.38, K = 7\n",
      "[Seed 129] TrainingError = 0.36, K = 6\n",
      "[Seed 130] TrainingError = 0.36, K = 7\n",
      "[Seed 131] TrainingError = 0.37, K = 6\n",
      "[Seed 132] TrainingError = 0.38, K = 7\n",
      "Optimal Model >> Seed 129, TrainingError = 0.36, K = 6\n",
      "Fit defragTrees time elapsed: 2241.5553 seconds\n",
      "\n",
      "defragTrees test accuracy\n",
      "Accuracy = 0.634293\n",
      "Coverage = 0.998940\n",
      "Overlap = 0.485432\n",
      "defragTrees benchmark\n",
      "0: Working on defragTrees for instance 12703\n",
      "10: Working on defragTrees for instance 8836\n",
      "20: Working on defragTrees for instance 2538\n",
      "30: Working on defragTrees for instance 4699\n",
      "40: Working on defragTrees for instance 14904\n",
      "50: Working on defragTrees for instance 8876\n",
      "60: Working on defragTrees for instance 7085\n",
      "70: Working on defragTrees for instance 1086\n",
      "80: Working on defragTrees for instance 9021\n",
      "90: Working on defragTrees for instance 8176\n",
      "100: Working on defragTrees for instance 3099\n",
      "110: Working on defragTrees for instance 9987\n",
      "120: Working on defragTrees for instance 149\n",
      "130: Working on defragTrees for instance 5771\n",
      "140: Working on defragTrees for instance 3314\n",
      "150: Working on defragTrees for instance 1397\n",
      "160: Working on defragTrees for instance 11628\n",
      "170: Working on defragTrees for instance 9749\n",
      "180: Working on defragTrees for instance 16135\n",
      "190: Working on defragTrees for instance 18007\n",
      "200: Working on defragTrees for instance 17353\n",
      "210: Working on defragTrees for instance 13680\n",
      "220: Working on defragTrees for instance 15498\n",
      "230: Working on defragTrees for instance 7420\n",
      "240: Working on defragTrees for instance 17643\n",
      "250: Working on defragTrees for instance 12789\n",
      "260: Working on defragTrees for instance 7256\n",
      "270: Working on defragTrees for instance 7847\n",
      "280: Working on defragTrees for instance 1657\n",
      "290: Working on defragTrees for instance 5933\n",
      "300: Working on defragTrees for instance 8649\n",
      "310: Working on defragTrees for instance 7917\n",
      "320: Working on defragTrees for instance 9229\n",
      "330: Working on defragTrees for instance 15383\n",
      "340: Working on defragTrees for instance 17945\n",
      "350: Working on defragTrees for instance 13480\n",
      "360: Working on defragTrees for instance 8843\n",
      "370: Working on defragTrees for instance 8582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380: Working on defragTrees for instance 4616\n",
      "390: Working on defragTrees for instance 9345\n",
      "400: Working on defragTrees for instance 1559\n",
      "410: Working on defragTrees for instance 15685\n",
      "420: Working on defragTrees for instance 14499\n",
      "430: Working on defragTrees for instance 7983\n",
      "440: Working on defragTrees for instance 8643\n",
      "450: Working on defragTrees for instance 2430\n",
      "460: Working on defragTrees for instance 2379\n",
      "470: Working on defragTrees for instance 6038\n",
      "480: Working on defragTrees for instance 2223\n",
      "490: Working on defragTrees for instance 14255\n",
      "500: Working on defragTrees for instance 10038\n",
      "510: Working on defragTrees for instance 9366\n",
      "520: Working on defragTrees for instance 5183\n",
      "530: Working on defragTrees for instance 15075\n",
      "540: Working on defragTrees for instance 18111\n",
      "550: Working on defragTrees for instance 6024\n",
      "560: Working on defragTrees for instance 18654\n",
      "570: Working on defragTrees for instance 4747\n",
      "580: Working on defragTrees for instance 18305\n",
      "590: Working on defragTrees for instance 4988\n",
      "600: Working on defragTrees for instance 13073\n",
      "610: Working on defragTrees for instance 908\n",
      "620: Working on defragTrees for instance 1404\n",
      "630: Working on defragTrees for instance 5896\n",
      "640: Working on defragTrees for instance 14393\n",
      "650: Working on defragTrees for instance 2722\n",
      "660: Working on defragTrees for instance 14947\n",
      "670: Working on defragTrees for instance 9609\n",
      "680: Working on defragTrees for instance 9856\n",
      "690: Working on defragTrees for instance 12399\n",
      "700: Working on defragTrees for instance 11251\n",
      "710: Working on defragTrees for instance 7687\n",
      "720: Working on defragTrees for instance 13692\n",
      "730: Working on defragTrees for instance 4364\n",
      "740: Working on defragTrees for instance 12531\n",
      "750: Working on defragTrees for instance 11764\n",
      "760: Working on defragTrees for instance 9940\n",
      "770: Working on defragTrees for instance 1562\n",
      "780: Working on defragTrees for instance 14400\n",
      "790: Working on defragTrees for instance 10658\n",
      "800: Working on defragTrees for instance 5219\n",
      "810: Working on defragTrees for instance 7907\n",
      "820: Working on defragTrees for instance 16289\n",
      "830: Working on defragTrees for instance 17299\n",
      "840: Working on defragTrees for instance 11385\n",
      "850: Working on defragTrees for instance 5167\n",
      "860: Working on defragTrees for instance 7725\n",
      "870: Working on defragTrees for instance 6496\n",
      "880: Working on defragTrees for instance 17362\n",
      "890: Working on defragTrees for instance 13774\n",
      "900: Working on defragTrees for instance 12834\n",
      "910: Working on defragTrees for instance 14444\n",
      "920: Working on defragTrees for instance 11369\n",
      "930: Working on defragTrees for instance 8414\n",
      "940: Working on defragTrees for instance 6920\n",
      "950: Working on defragTrees for instance 5881\n",
      "960: Working on defragTrees for instance 2220\n",
      "970: Working on defragTrees for instance 3172\n",
      "980: Working on defragTrees for instance 10872\n",
      "990: Working on defragTrees for instance 12349\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# do the model tuning from scratch?\n",
    "override_tuning = False\n",
    "\n",
    "# Optional Memory and Computation Cost Management\n",
    "# CHIRPS is time economical but memory intensive to compute for lots of instances at once.\n",
    "forest_walk_async=True\n",
    "explanation_async=True\n",
    "n_cores = mp.cpu_count()-2\n",
    "\n",
    "# How many instances from the test set do you want to explain?\n",
    "# A number bigger than the test set will be interpreted as 'all'\n",
    "n_instances = 1000\n",
    "start_instance = 0 # here can opt to start at a specific instance, diagnostic if something crashes\n",
    "\n",
    "# CHOOSE ONE\n",
    "# model = 'RandomForest'\n",
    "# model = 'AdaBoost1' # SAMME\n",
    "# model = 'AdaBoost2' # SAMME.R\n",
    "model = 'GBM'\n",
    "\n",
    "# CHOOSE ONE OR MORE\n",
    "do_CHIRPS = False\n",
    "do_Anchors = False\n",
    "do_dfrgTrs = True\n",
    "do_lore = False\n",
    "\n",
    "# list the dataset constructors you want to include\n",
    "datasets = [\n",
    "            ds.adult,\n",
    "#             ds.bankmark,\n",
    "#             ds.car,\n",
    "#             ds.cardio,\n",
    "#             ds.credit,\n",
    "#             ds.german,\n",
    "#             ds.lending_tiny_samp,\n",
    "#             ds.nursery,\n",
    "            ds.rcdv,\n",
    "           ]\n",
    "\n",
    "# location to save results\n",
    "project_dir = '/datadisk/whiteboxing/2020GBM_copy'\n",
    "# project_dir = 'V:\\\\whiteboxing\\\\2020' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "# project_dir = 'C:\\\\Users\\\\Crutt\\\\Documents\\\\whiteboxing\\\\2020'\n",
    "\n",
    "# set the random_state for various tasks. This is not the same as random.seed(), not system wide.\n",
    "random_state_splits = 123 # change this if you want to try different splits of the data into test / train\n",
    "random_state_rf = 123 # change this if you want to try with different forest construction\n",
    "random_state_exp = 123 # change this if you want to try with different runs of the explainer algorithm (affects bootstrap eval)\n",
    "\n",
    "# To standardise train-test splitting across environments - writes external files\n",
    "rp.export_data_splits(datasets=datasets, project_dir=project_dir, random_state_splits=random_state_splits)\n",
    "\n",
    "# How much messaging to print to the screen?\n",
    "verbose = True\n",
    "\n",
    "# CHIRPS default parameters - see papers for details\n",
    "merging_bootstraps = 20 # how many training bootstraps to test improvement in growing rule?\n",
    "pruning_bootstraps = 20 # how many training bootstraps to test deterioration in pruning rule?\n",
    "delta = 0.2 # pruning deterioration tolerance paramater\n",
    "\n",
    "# this is here if you want to pass parameters to the methods\n",
    "def benchmark_wrapper(do_CHIRPS=do_CHIRPS, do_Anchors=do_Anchors, do_dfrgTrs=do_dfrgTrs, do_lore=do_lore, **control):\n",
    "\n",
    "    if do_CHIRPS:\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "    \n",
    "    if do_Anchors:\n",
    "        control.update({'method' : 'Anchors'})\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "    if do_dfrgTrs:\n",
    "        control.update({'method' : 'defragTrees',\n",
    "                        'Kmax' : 10, 'restart' : 10, 'maxitr' : 100})\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "\n",
    "    if do_lore:\n",
    "        control.update({'method' : 'lore',\n",
    "                        'max_popsize' : 1000})\n",
    "        rp.do_benchmarking(benchmark_items, verbose, **control)\n",
    "\n",
    "# this is here to pass parameters to the model training and further parameters to CHIRPS\n",
    "tuning = {'override' : override_tuning}\n",
    "if model == 'RandomForest':\n",
    "    tuning.update({'grid' : {'n_estimators': [(i + 1) * 200 for i in range(8)],\n",
    "                            'max_depth' : [32]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                           random_state=random_state_rf,\n",
    "                                           random_state_splits=random_state_splits,\n",
    "                                           do_raw=True, do_discretise=do_Anchors,\n",
    "                                           start_instance=start_instance,\n",
    "                                           verbose=verbose, n_cores=n_cores)\n",
    "\n",
    "    kwargs = {'support_paths' : 0.2, 'alpha_paths' : 0.5, 'disc_path_bins' : 8,\n",
    "             'score_func' : 1, 'weighting' : 'kldiv',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps,\n",
    "             'precis_threshold' : 0.99,\n",
    "             'delta' : delta}\n",
    " \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'explanation_async' : explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "    \n",
    "elif model == \"AdaBoost1\":\n",
    "    algo = 'SAMME'\n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)],\n",
    "                             'algorithm': [algo]}}) \n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                       random_state=random_state_rf,\n",
    "                                       random_state_splits=random_state_splits,\n",
    "                                       do_raw=(do_CHIRPS or do_dfrgTrs or do_lore), do_discretise=do_Anchors,\n",
    "                                       start_instance=start_instance,\n",
    "                                       verbose=verbose, n_cores=n_cores)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "             'support_paths' : 0.01, 'alpha_paths' : 0.0,\n",
    "             'disc_path_bins' : 8, 'disc_path_eqcounts' : False,\n",
    "             'score_func' : 1, 'weighting' : 'chisq',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps, 'delta' : delta}\n",
    " \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'explanation_async' : explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "    \n",
    "elif model == 'AdaBoost2':\n",
    "    algo = 'SAMME.R'\n",
    "    max_depth = [i for i in range(1, 5)]\n",
    "    tuning.update({'grid' : {'base_estimator' : [rp.DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                            'n_estimators': [(i + 1) * 200 for i in range(8)],\n",
    "                             'algorithm': [algo]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                       random_state=random_state_rf,\n",
    "                                       random_state_splits=random_state_splits,\n",
    "                                       do_raw=(do_CHIRPS or do_dfrgTrs or do_lore), do_discretise=do_Anchors,\n",
    "                                       start_instance=start_instance,\n",
    "                                       verbose=verbose, n_cores=n_cores)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "                 'support_paths' : 0.005, 'alpha_paths' : 0.0,\n",
    "                 'disc_path_bins' : 8, 'disc_path_eqcounts' : False,\n",
    "                 'score_func' : 1, 'weighting' : 'chisq',\n",
    "                 'merging_bootstraps' : merging_bootstraps,\n",
    "                 'pruning_bootstraps' : pruning_bootstraps,\n",
    "                 'which_trees' : 'majority',\n",
    "                 'delta' : delta}\n",
    "    \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'explanation_async' : explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "    \n",
    "else: # GBM - not fully implemented yet\n",
    "    tuning.update({'grid' : {'subsample' : [0.5],\n",
    "                            'n_estimators': [i * 200 for i in range(1, 9)],\n",
    "                            'max_depth' : [i for i in range(1, 5)],\n",
    "                            'learning_rate': np.full(4, 10.0)**[i for i in range(-3, 1)]}})\n",
    "    benchmark_items = rp.benchmarking_prep(datasets, model, tuning, project_dir,\n",
    "                                       random_state=random_state_rf,\n",
    "                                       random_state_splits=random_state_splits,\n",
    "                                       do_raw=(do_CHIRPS or do_dfrgTrs or do_lore), do_discretise=do_Anchors,\n",
    "                                       start_instance=start_instance,\n",
    "                                       verbose=verbose, n_cores=n_cores)\n",
    "    \n",
    "    kwargs = {'paths_lengths_threshold' : 5,\n",
    "             'support_paths' : 0.05, 'alpha_paths' : 0.0,\n",
    "             'disc_path_bins' : 4, 'disc_path_eqcounts' : True,\n",
    "             'score_func' : 1, 'weighting' : 'kldiv',\n",
    "             'merging_bootstraps' : merging_bootstraps,\n",
    "             'pruning_bootstraps' : pruning_bootstraps,\n",
    "             'which_trees' : 'targetclass',\n",
    "             'delta' : 0.2}\n",
    "    \n",
    "    control = {'method' : 'CHIRPS', 'model' : model,\n",
    "                'n_instances' : n_instances,\n",
    "                'random_state' : random_state_exp,\n",
    "                'kwargs' : kwargs,\n",
    "                'forest_walk_async' : forest_walk_async,\n",
    "                'explanation_async' : explanation_async,\n",
    "                'n_cores' : n_cores}\n",
    "    \n",
    "    benchmark_wrapper(do_CHIRPS, do_Anchors, do_dfrgTrs, do_lore, **control)\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
