{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up notebook and load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for notebook plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# load what we need\n",
    "import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "import CHIRPS.structures as strcts\n",
    "import CHIRPS.routines as rt\n",
    "import CHIRPS.reproducible as rp\n",
    "import CHIRPS.boosting_scratch as bs\n",
    "from IPython.display import Image\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from pydotplus import graph_from_dot_data\n",
    "\n",
    "# demo datasets that ship with package. all from UCI unless stated otherwise\n",
    "# import CHIRPS.datasets as ds\n",
    "# ds.adult_data, ds.adult_samp_data, ds.adult_small_samp_data Large dataset ships with manageable sub samples\n",
    "# ds.bankmark_data, ds.bankmark_samp_data\n",
    "# ds.car_data\n",
    "# ds.cardio_data this is the cardiotocography dataset\n",
    "# ds.credit_data\n",
    "# ds.german_data\n",
    "# ds.lending_data, ds.lending_samp_data, ds.lending_small_samp_data, ds.lending_tiny_samp_data from Kaggle. see datasets_from_source file for links\n",
    "# ds.nursery_data, ds.nursery_samp_data\n",
    "# ds.rcdv_data, ds.rcdv_samp_data from US government see datasets_from_source file for links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common config - can be ommitted if defaults are OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'V:\\\\whiteboxing\\\\examples' # defaults to a directory \"whiteboxing\" in the working directory\n",
    "# project_dir = 'C:\\\\Users\\\\Crutt\\\\Documents\\\\whiteboxing\\\\examples'\n",
    "random_state_splits = 123 # one off for splitting the data into test / train\n",
    "random_state = 123 # for everything else - e.g. building a new rf with same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Random Forest Model to Predict and Explain\n",
    "First, a wrapper is created for the dataset. Use one that ships with the package, or create your own.\n",
    "Then split the data into training and (hold out) test set using the convenience functions in the package. These return an object that contain the split data in various representations, such as Pandas DataFrames and encoded, sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# load one of the included datasets\n",
    "# project_dir will default to directory name CHIRPS in the working directory if not given\n",
    "# random_state will default to 123\n",
    "mydata = rp.datasets[0](random_state=random_state, project_dir=project_dir)\n",
    "meta_data = mydata.get_meta()\n",
    "save_path = meta_data['get_save_path']()\n",
    "\n",
    "# split the data. here using a basic sampling method.\n",
    "# the returned object is a wrapper class that contains\n",
    "# the train and test splits for X and y\n",
    "\n",
    "# also the the encoded versions of X_train and X_test that the rf will use\n",
    "# this is because we prefer onehot encoded over allowing categorical vars to be represented as integer\n",
    "# scikit would treat these as ordinal, which is inappropriate\n",
    "\n",
    "# also some meta-data: priors for y, the indexes from the input data\n",
    "\n",
    "# also some convenience functions for leave-one-out testing\n",
    "\n",
    "# train test split - one off hard-coded random state.\n",
    "# random state can be ommitted \n",
    "# and will default to the state held in the dataset container\n",
    "# which defaults to 123 if ommitted in the constructor\n",
    "train_index, test_index = mydata.get_tt_split_idx(random_state=random_state_splits)\n",
    "# optionally, indexes can be ommitted and will default to scikit's train_test_split method\n",
    "tt = mydata.tt_split(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over-riding previous tuning parameters. New grid tuning... (please wait)\n",
      "\n",
      "Finding best params with 10-fold CV\n",
      "0.847 (+/-0.054) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200}\n",
      "0.844 (+/-0.047) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400}\n",
      "0.844 (+/-0.041) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600}\n",
      "0.845 (+/-0.048) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 800}\n",
      "0.843 (+/-0.050) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1000}\n",
      "0.839 (+/-0.048) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1200}\n",
      "0.844 (+/-0.046) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1400}\n",
      "0.847 (+/-0.052) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600}\n",
      "0.822 (+/-0.060) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200}\n",
      "0.822 (+/-0.046) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400}\n",
      "0.819 (+/-0.051) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600}\n",
      "0.819 (+/-0.064) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 800}\n",
      "0.820 (+/-0.052) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1000}\n",
      "0.819 (+/-0.054) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1200}\n",
      "0.818 (+/-0.055) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1400}\n",
      "0.816 (+/-0.058) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600}\n",
      "0.815 (+/-0.060) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200}\n",
      "0.813 (+/-0.057) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400}\n",
      "0.827 (+/-0.051) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600}\n",
      "0.828 (+/-0.054) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 800}\n",
      "0.829 (+/-0.041) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1000}\n",
      "0.824 (+/-0.041) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1200}\n",
      "0.830 (+/-0.041) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1400}\n",
      "0.829 (+/-0.040) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600}\n",
      "0.821 (+/-0.043) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200}\n",
      "0.827 (+/-0.050) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 400}\n",
      "0.827 (+/-0.040) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 600}\n",
      "0.829 (+/-0.024) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 800}\n",
      "0.833 (+/-0.031) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1000}\n",
      "0.826 (+/-0.040) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1200}\n",
      "0.831 (+/-0.031) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1400}\n",
      "0.832 (+/-0.036) for {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 1600}\n",
      "CV time: 1099.58517669\n",
      "\n",
      "Tuning time elapsed: 1099.5866 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best OOB Accuracy Estimate during tuning: 0.8473\n",
      "Best parameters: {'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 200, 'random_state': 123}\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[517  47]\n",
      " [ 63 106]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.92 0.08]\n",
      " [0.37 0.63]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model\n",
    "model='AdaBoost'\n",
    "algo = 'SAMME.R'\n",
    "max_depth = [i for i in range(1, 5)]\n",
    "tuning_grid = {'base_estimator' : [DecisionTreeClassifier(max_depth=d) for d in max_depth],\n",
    "                        'n_estimators': [(i + 1) * 200 for i in range(8)], 'algorithm': [algo]}\n",
    "\n",
    "rf = rp.forest_prep(ds_container=tt,\n",
    "                    meta_data=meta_data,\n",
    "                    override_tuning=True,\n",
    "                    model=model,\n",
    "                    tuning_grid=tuning_grid,\n",
    "                    save_path=save_path,\n",
    "                    plot_cm=True, plot_cm_norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control for async processes - each tree walk can be done in its own core\n",
    "# and so can each explanation (e.g. rule conditions merge by hill-climbing)\n",
    "# these will default to false if not passed explicitly to the explainer function\n",
    "# on a multi-core machine there should be a good speed up for large batches\n",
    "# when the batch_size advantage exceeds the overhead of setting up multi-processing\n",
    "# timings will be printed to screen so you can see if it helps\n",
    "forest_walk_async=True\n",
    "chirps_explanation_async=True\n",
    "\n",
    "# how many instances to explain in total from a test/unseen set\n",
    "n_instances = 1\n",
    "\n",
    "# this will normalise the above parameters to the size of the dataset\n",
    "n_instances = rt.n_instance_ceiling(ds_container=tt, n_instances=n_instances)\n",
    "\n",
    "# this gets the next batch out of the data_split_container according to the required number of instances\n",
    "# all formats can be extracted, depending on the requirement\n",
    "# unencoded, encoded (sparse matrix is the type returned by scikit), ordinary dense matrix also available\n",
    "instances, _, instances_enc, instances_enc_matrix, labels = tt.get_next(n_instances, which_split='test') # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=1400, random_state=123)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for e in rf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walking forest for 1 instances... (please wait)\n",
      "Forest Walk with async = True\n",
      "Forest Walk time elapsed: 1.0486 seconds\n"
     ]
    }
   ],
   "source": [
    "# get all the model predictions for the test instance(s) we're looking at\n",
    "preds_idx = labels.index\n",
    "preds = rf.predict(X=instances_enc)\n",
    "\n",
    "f_walker = strcts.forest_walker(forest = rf, meta_data=meta_data)\n",
    "\n",
    "print('Walking forest for ' + str(len(labels)) + ' instances... (please wait)')\n",
    "\n",
    "# set the timer\n",
    "forest_walk_start_time = timeit.default_timer()\n",
    "\n",
    "# do the walk - returns a batch_paths_container (even for just one instance)\n",
    "# requires the X instances in a matrix (dense, ordinary numpy matrix) - this is available in the data_split_container\n",
    "bp_container = f_walker.forest_walk(instances = instances_enc_matrix\n",
    "                        , labels = preds # we're explaining the prediction, not the true label!\n",
    "                        , forest_walk_async = forest_walk_async)\n",
    "\n",
    "# stop the timer\n",
    "forest_walk_end_time = timeit.default_timer()\n",
    "forest_walk_elapsed_time = forest_walk_end_time - forest_walk_start_time\n",
    "\n",
    "print('Forest Walk with async = ' + str(forest_walk_async))\n",
    "print('Forest Walk time elapsed:', \"{:0.4f}\".format(forest_walk_elapsed_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get what the model predicts on the training sample\n",
    "sample_labels = rf.predict(tt.X_train_enc)\n",
    "\n",
    "# build CHIRPS and a rule for each instance represented in the batch paths container\n",
    "CHIRPS = strcts.batch_CHIRPS_explainer(bp_container,\n",
    "                                forest=rf,\n",
    "                                sample_instances=tt.X_train_enc, # any representative sample can be used\n",
    "                                # sample_labels=tt.y_train,  # any representative sample can be used\n",
    "                                sample_labels=sample_labels,\n",
    "                                meta_data=meta_data)\n",
    "\n",
    "print('Running CHIRPS on a batch of ' + str(len(labels)) + ' instances... (please wait)')\n",
    "# start a timer\n",
    "ce_start_time = timeit.default_timer()\n",
    "\n",
    "CHIRPS.batch_run_CHIRPS(chirps_explanation_async=chirps_explanation_async,\n",
    "                        alpha_paths=0.9,\n",
    "                        support_paths=0.01,\n",
    "                        score_func=5,\n",
    "                        disc_path_bins=4,\n",
    "                        target_classes=preds,\n",
    "                        merging_bootstraps=20,\n",
    "                        pruning_bootstraps=20,\n",
    "                        delta=0.1,\n",
    "                        weighting='chisq')\n",
    "\n",
    "ce_end_time = timeit.default_timer()\n",
    "ce_elapsed_time = ce_end_time - ce_start_time\n",
    "print('CHIRPS time elapsed:', \"{:0.4f}\".format(ce_elapsed_time), 'seconds')\n",
    "print('CHIRPS with async = ' + str(chirps_explanation_async))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#c = bp_container.get_CHIRPS_runner(0, meta_data)\n",
    "#c.repeat_weighted_paths()\n",
    "#CHIRPS.CHIRPS_explainers[0].paths\n",
    "#np.round(CHIRPS.CHIRPS_explainers[0].paths_weights * 1/min(CHIRPS.CHIRPS_explainers[0].paths_weights))\n",
    "#CHIRPS.CHIRPS_explainers[0].paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate over all the test instances to determine the various scores using leave-one-out testing\n",
    "print('evaluating found explanations')\n",
    "print()\n",
    "results_start_time = timeit.default_timer()\n",
    "\n",
    "rt.evaluate_CHIRPS_explainers(CHIRPS, tt, labels.index, # for batch runs: tt.y_test.index,\n",
    "                              forest=rf,\n",
    "                              meta_data=meta_data,\n",
    "                              model=model,\n",
    "                              eval_start_time=results_start_time,\n",
    "                              print_to_screen=True, # set True when running single instances\n",
    "                              eval_alt_labelings=True,\n",
    "                              eval_rule_complements=True,\n",
    "                              save_results_path=save_path,\n",
    "                              dataset_name='test',\n",
    "                              save_results_file='CHIRPS' + '_rnst_' + str(random_state),\n",
    "                              save_CHIRPS=False)\n",
    "\n",
    "results_end_time = timeit.default_timer()\n",
    "results_elapsed_time = results_end_time - results_start_time\n",
    "print('CHIRPS batch results eval time elapsed:', \"{:0.4f}\".format(results_elapsed_time), 'seconds')\n",
    "# this completes the CHIRPS runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(rf.predict_proba(instances_enc))\n",
    "# print([i for i in rf.staged_predict_proba(instances_enc)])\n",
    "# print([i for i in rf.staged_decision_function(instances_enc)])\n",
    "print(math.exp(rf.decision_function(instances_enc)))\n",
    "print(math.exp(rf.decision_function(instances_enc)/ (1 + rf.decision_function(instances_enc))))\n",
    "rf.estimators_[0].predict_proba(instances_enc)\n",
    "rf.classes_.take([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_pred = [(estimator.predict(instances_enc) == rf.classes_) for estimator in rf.estimators_]\n",
    "# print(r_pred)\n",
    "rw_pred = [(estimator.predict(instances_enc) == rf.classes_).T * w for estimator, w in zip(rf.estimators_, rf.estimator_weights_)]\n",
    "# print(rw_pred)\n",
    "pred = sum((estimator.predict(instances_enc) == rf.classes_).T * w\n",
    "           for estimator, w in zip(rf.estimators_,\n",
    "                                   rf.estimator_weights_))\n",
    "print(pred)\n",
    "print(rf.estimator_weights_.sum())\n",
    "pred /= rf.estimator_weights_.sum()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = rf.n_classes_\n",
    "print(n_classes)\n",
    "proba = sum(estimator.predict_proba(instances_enc) * w\n",
    "                        for estimator, w in zip(rf.estimators_,\n",
    "                                                rf.estimator_weights_))\n",
    "\n",
    "print(proba)\n",
    "proba /= rf.estimator_weights_.sum()\n",
    "print(proba)\n",
    "proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "print(proba)\n",
    "normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "normalizer[normalizer == 0.0] = 1.0\n",
    "print(normalizer)\n",
    "proba /= normalizer\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "    log_proba = np.log(proba)\n",
    "    \n",
    "    print(proba)\n",
    "    print((log_proba - (1. / n_classes)\n",
    "                              * log_proba.sum(axis=1)[:, np.newaxis]))\n",
    "    \n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                              * log_proba.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how is GB working to calculate the predict function\n",
    "import numpy as np\n",
    "n = 300\n",
    "gbpreds = [gb.predict(X_test[i])[0] for i in range(n)]\n",
    "reg_tots = np.empty(n)\n",
    "for i in range(n):\n",
    "    reg_tots[i] = sum([gb.estimators_[j][0].predict(X_test[i])[0] for j in range(ne)])\n",
    "\n",
    "mn = np.inf\n",
    "mx = -np.inf\n",
    "for pred, tot in zip(gbpreds, reg_tots):\n",
    "    if pred == 0:\n",
    "        if mx < tot:\n",
    "            mx = tot\n",
    "    if pred == 1:\n",
    "        if mn > tot:\n",
    "            mn = tot\n",
    "            \n",
    "print(mx, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing unseen data\n",
    "\n",
    "Again note:\n",
    "test set has never been \"seen\" by random forest during training\n",
    "test set has been only used to assess model (random forest) accuracy - no additional tuning after this\n",
    "test set has not be involved in generating the explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional: memory and computation cost management\n",
    "#### CHIRPS is time economical but memory intensive to compute for lots of instances at once\n",
    "option 1: choose a smaller number of instances to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control for async processes - each tree walk can be done in its own core\n",
    "# and so can each explanation (e.g. rule conditions merge by hill-climbing)\n",
    "# these will default to false if not passed explicitly to the explainer function\n",
    "# on a multi-core machine there should be a good speed up for large batches\n",
    "# when the batch_size advantage exceeds the overhead of setting up multi-processing\n",
    "# timings will be printed to screen so you can see if it helps\n",
    "forest_walk_async=False\n",
    "chirps_explanation_async=False\n",
    "\n",
    "# the number of instances can be controlled by\n",
    "# batch_size - how many instances to explain at one time\n",
    "batch_size = 1\n",
    "# how many instances to explain in total from a test/unseen set\n",
    "n_instances = 1\n",
    "\n",
    "# this will normalise the above parameters to the size of the dataset\n",
    "n_instances, n_batches = rt.batch_instance_ceiling(ds_container=tt, n_instances=n_instances, batch_size=batch_size)\n",
    "\n",
    "# this gets the next batch out of the data_split_container according to the required number of instances\n",
    "# all formats can be extracted, depending on the requirement\n",
    "# unencoded, encoded (sparse matrix is the type returned by scikit), ordinary dense matrix also available\n",
    "instances, _, instances_enc, instances_enc_matrix, labels = tt.get_next(batch_size, which_split='test') # default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 2: just run the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances = tt.X_test; instances_enc = tt.X_test_enc; instances_enc_matrix = tt.X_test_enc_matrix; labels = tt.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions from the decision forest on the unseen data\n",
    "Important point, no compromise on model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the model predictions for the test instance(s) we're looking at\n",
    "preds_idx = labels.index\n",
    "preds = rf.predict(X=instances_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHIRPS Step 1:\n",
    "## Extract Tree Prediction Paths\n",
    "### Fit a forest_walker object to the dataset and decision forest\n",
    "This is a wrapper will extracts the paths of all the given instances. For CHIRPS, we want a large sample. The whole training set or other representative sample will do.\n",
    "\n",
    "It can also report interesting statistics (treating the forest as a set of random tree-structured variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper object needs the decision forest itself and the dataset meta data (we have a convenience function for this)\n",
    "f_walker = strcts.forest_walker(forest = rf, meta_data=meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the work of extracting all the paths for each instance is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Walking forest for ' + str(len(labels)) + ' instances... (please wait)')\n",
    "\n",
    "# set the timer\n",
    "forest_walk_start_time = timeit.default_timer()\n",
    "\n",
    "# do the walk - returns a batch_paths_container (even for just one instance)\n",
    "# requires the X instances in a matrix (dense, ordinary numpy matrix) - this is available in the data_split_container\n",
    "bp_container = f_walker.forest_walk(instances = instances_enc_matrix\n",
    "                        , labels = preds # we're explaining the prediction, not the true label!\n",
    "                        , forest_walk_async = forest_walk_async)\n",
    "\n",
    "# stop the timer\n",
    "forest_walk_end_time = timeit.default_timer()\n",
    "forest_walk_elapsed_time = forest_walk_end_time - forest_walk_start_time\n",
    "\n",
    "print('Forest Walk with async = ' + str(forest_walk_async))\n",
    "print('Forest Walk time elapsed:', \"{:0.4f}\".format(forest_walk_elapsed_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHIRPS Steps 2-4: \n",
    "## Freqent pattern mining of paths.\n",
    "## Score and sort mined path segments.\n",
    "## Merge path segments into one rule.\n",
    "\n",
    "This is a wrapper object that will execute steps 2-4 on all the instance-paths in the batch_paths_container.\n",
    "\n",
    "Note that true_divide warnings are OK. It just means that a continuous variable is unbounded in some way i.e. no greater/less than discontinuity is used in the CHIRPS explanation.\n",
    "\n",
    "Note also, here we are using the training set to create the explainers. We could use a different dataset as long as it is representative of the training set that built the decision forest. Most important that we don't use the dataset that we wish to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get what the model predicts on the training sample\n",
    "sample_labels = rf.predict(tt.X_train_enc)\n",
    "\n",
    "# build CHIRPS and a rule for each instance represented in the batch paths container\n",
    "CHIRPS = strcts.batch_CHIRPS_explainer(bp_container,\n",
    "                                forest=rf,\n",
    "                                sample_instances=tt.X_train_enc, # any representative sample can be used\n",
    "                                # sample_labels=tt.y_train,  # any representative sample can be used\n",
    "                                sample_labels=sample_labels,\n",
    "                                meta_data=meta_data)\n",
    "\n",
    "print('Running CHIRPS on a batch of ' + str(len(labels)) + ' instances... (please wait)')\n",
    "# start a timer\n",
    "ce_start_time = timeit.default_timer()\n",
    "\n",
    "CHIRPS.batch_run_CHIRPS(chirps_explanation_async=chirps_explanation_async,\n",
    "                        alpha_paths=0.9,\n",
    "                        support_paths=0.1,\n",
    "                        score_func=5,\n",
    "                        disc_path_bins=4,\n",
    "                        target_classes=preds,\n",
    "                        merging_bootstraps=20,\n",
    "                        pruning_bootstraps=20,\n",
    "                        delta=0.1,\n",
    "                        weighting='chisq')\n",
    "\n",
    "ce_end_time = timeit.default_timer()\n",
    "ce_elapsed_time = ce_end_time - ce_start_time\n",
    "print('CHIRPS time elapsed:', \"{:0.4f}\".format(ce_elapsed_time), 'seconds')\n",
    "print('CHIRPS with async = ' + str(chirps_explanation_async))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing and Evaluating CHIRPS explanations\n",
    "Evaluation is done using unseen data to see how well the explanations generalise. The data_split_container object (tt) has a  leave-one-out function that is used during the routine to ensure that the datum we are explaining is excluded from the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate over all the test instances to determine the various scores using leave-one-out testing\n",
    "print('evaluating found explanations')\n",
    "print()\n",
    "results_start_time = timeit.default_timer()\n",
    "\n",
    "rt.evaluate_CHIRPS_explainers(CHIRPS, tt, tt.y_test.index,\n",
    "                              forest=rf,\n",
    "                              meta_data=meta_data,\n",
    "                              eval_start_time=results_start_time,\n",
    "                              print_to_screen=True, # set True when running single instances\n",
    "                              eval_alt_labelings=True,\n",
    "                              eval_rule_complements=True,\n",
    "                              save_results_path=save_path,\n",
    "                              dataset_name='test',\n",
    "                              save_results_file='CHIRPS' + '_rnst_' + str(random_state),\n",
    "                              save_CHIRPS=False)\n",
    "\n",
    "results_end_time = timeit.default_timer()\n",
    "results_elapsed_time = results_end_time - results_start_time\n",
    "print('CHIRPS batch results eval time elapsed:', \"{:0.4f}\".format(results_elapsed_time), 'seconds')\n",
    "# this completes the CHIRPS runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "forest = rf\n",
    "train_pred_labels = Series(forest.predict(tt.X_train_enc), index = tt.y_train.index)\n",
    "CHIRPS.CHIRPS_explainers[0].evaluate_rule(rule='pruned', sample_instances=tt.X_train_enc, sample_labels=train_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIRPS.CHIRPS_explainers[0].posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict_proba(X=instances_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(preds) - 0.5 * np.log(preds).sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sum(_samme_proba(estimator, n_classes, X)\n",
    "                       for estimator in self.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sum(_samme_proba(estimator, 2, instances_enc)\n",
    "                       for estimator in rf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(_samme_proba(rf.estimators_[0], 2, instances_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.estimators_[0].predict(instances_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "[math.exp(ew)/(1+math.exp(ew)) - 0.5 for ew in rf.estimator_weights_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.estimator_errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.decision_function(instances_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(instances_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict_proba(instances_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le_dict = LabelEncoder().fit(mydata.data['workclass'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>lfnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educationnum</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>lcapitalgain</th>\n",
       "      <th>lcapitalloss</th>\n",
       "      <th>hoursperweek</th>\n",
       "      <th>nativecountry</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>12.345543</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.501227</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.272824</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.702107</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>10.897646</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>12.167883</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.194471</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>11.878346</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.203755</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.937736</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.071284</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>46</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.570987</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>11.654668</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.601902</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>12.735197</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.125047</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.378045</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.184895</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.394811</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>12.819719</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>16</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.594616</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.335806</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>42</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>11.890471</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>El-Salvador</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.434224</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.580604</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.698788</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.077810</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>12.720374</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>45</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.166791</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.659401</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.792449</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>48</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>11.717848</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>27</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>12.361378</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>68</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>11.744069</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.138073</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.335068</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>51</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>11.338298</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.036174</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.913525</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>16</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>21</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.670310</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>45</td>\n",
       "      <td>Private</td>\n",
       "      <td>11.793636</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>57</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.791771</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>46</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.480316</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.400693</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>59</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.164625</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>79</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>12.185727</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>10.507202</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>62</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.270023</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>29</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.315613</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>33</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.262042</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>48</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.874012</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>10.793619</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>12.716189</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.781359</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>60</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.400121</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>26</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>11.358351</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Amer-Indian-Eskimo</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.952543</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>3</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>26</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.472513</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.332221</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>19</td>\n",
       "      <td>Private</td>\n",
       "      <td>10.837716</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>12.121086</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>South</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>41</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>11.243712</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>9.617471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2442 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age         workclass    lfnlwgt     education  educationnum  \\\n",
       "0      51       Federal-gov  12.345543  Some-college            10   \n",
       "1      38           Private  11.501227     Bachelors            13   \n",
       "2      42           Private  12.272824  Some-college            10   \n",
       "3      39           Private  11.702107       HS-grad             9   \n",
       "4      22           Private  10.897646  Some-college            10   \n",
       "5      38  Self-emp-not-inc  12.167883       HS-grad             9   \n",
       "6      54           Private  12.194471     Bachelors            13   \n",
       "7      47  Self-emp-not-inc  11.878346  Some-college            10   \n",
       "8      34           Private  12.203755       HS-grad             9   \n",
       "9      45           Private  11.937736       HS-grad             9   \n",
       "10     17           Private  11.071284          11th             7   \n",
       "11     46           Private  12.570987     Bachelors            13   \n",
       "12     21         Local-gov  11.654668  Some-college            10   \n",
       "13     30         Local-gov  12.735197       HS-grad             9   \n",
       "14     42           Private  12.125047  Some-college            10   \n",
       "15     30           Private  12.378045       Masters            14   \n",
       "16     27           Private  12.184895     Bachelors            13   \n",
       "17     51           Private  12.394811     Bachelors            13   \n",
       "18     31  Self-emp-not-inc  12.819719     Doctorate            16   \n",
       "19     27           Private  11.594616       HS-grad             9   \n",
       "20     39           Private  12.335806     Bachelors            13   \n",
       "21     42  Self-emp-not-inc  11.890471  Some-college            10   \n",
       "22     19           Private  12.434224       HS-grad             9   \n",
       "23     36           Private  12.580604       HS-grad             9   \n",
       "24     23           Private  12.698788  Some-college            10   \n",
       "25     43           Private  11.077810  Some-college            10   \n",
       "26     27         Local-gov  12.720374  Some-college            10   \n",
       "27     45           Private  12.166791       HS-grad             9   \n",
       "28     27           Private  12.659401       HS-grad             9   \n",
       "29     19           Private  11.792449       HS-grad             9   \n",
       "...   ...               ...        ...           ...           ...   \n",
       "2412   48       Federal-gov  11.717848  Some-college            10   \n",
       "2413   27         Local-gov  12.361378          11th             7   \n",
       "2414   68      Self-emp-inc  11.744069          11th             7   \n",
       "2415   23           Private  12.335068  Some-college            10   \n",
       "2416   51         State-gov  11.338298     Bachelors            13   \n",
       "2417   30           Private  12.036174       Masters            14   \n",
       "2418   44           Private  11.913525     Doctorate            16   \n",
       "2419   21           Private  12.670310  Some-college            10   \n",
       "2420   45           Private  11.793636     Bachelors            13   \n",
       "2421   57           Private  12.791771     Bachelors            13   \n",
       "2422   46           Private  12.480316   Prof-school            15   \n",
       "2423   40           Private  12.400693       HS-grad             9   \n",
       "2424   59           Private  12.164625       HS-grad             9   \n",
       "2425   79      Self-emp-inc  12.185727     Bachelors            13   \n",
       "2426   30           Private  10.507202     Bachelors            13   \n",
       "2427   62           Private  12.270023       7th-8th             4   \n",
       "2428   29           Private  12.315613          11th             7   \n",
       "2429   33           Private  12.262042     Bachelors            13   \n",
       "2430   48           Private  12.874012          10th             6   \n",
       "2431   36           Private  10.793619       HS-grad             9   \n",
       "2432   38      Self-emp-inc  12.716189       HS-grad             9   \n",
       "2433   54           Private  12.781359  Some-college            10   \n",
       "2434   60           Private  12.400121       7th-8th             4   \n",
       "2435   26         Local-gov  11.358351       HS-grad             9   \n",
       "2436   34           Private  12.952543       5th-6th             3   \n",
       "2437   26           Private  12.472513  Some-college            10   \n",
       "2438   36           Private  12.332221     Bachelors            13   \n",
       "2439   19           Private  10.837716  Some-college            10   \n",
       "2440   22           Private  12.121086     Assoc-voc            11   \n",
       "2441   41      Self-emp-inc  11.243712    Assoc-acdm            12   \n",
       "\n",
       "           maritalstatus         occupation    relationship  \\\n",
       "0               Divorced     Prof-specialty   Not-in-family   \n",
       "1               Divorced              Sales   Not-in-family   \n",
       "2     Married-civ-spouse       Craft-repair         Husband   \n",
       "3     Married-civ-spouse              Sales         Husband   \n",
       "4          Never-married     Prof-specialty   Not-in-family   \n",
       "5     Married-civ-spouse       Craft-repair         Husband   \n",
       "6     Married-civ-spouse    Exec-managerial         Husband   \n",
       "7     Married-civ-spouse              Sales         Husband   \n",
       "8               Divorced     Prof-specialty       Unmarried   \n",
       "9     Married-civ-spouse  Machine-op-inspct         Husband   \n",
       "10         Never-married      Other-service       Own-child   \n",
       "11         Never-married    Exec-managerial       Unmarried   \n",
       "12         Never-married      Other-service   Not-in-family   \n",
       "13    Married-civ-spouse      Other-service         Husband   \n",
       "14    Married-civ-spouse       Adm-clerical         Husband   \n",
       "15    Married-civ-spouse     Prof-specialty         Husband   \n",
       "16         Never-married    Exec-managerial   Not-in-family   \n",
       "17    Married-civ-spouse    Exec-managerial         Husband   \n",
       "18         Never-married     Prof-specialty       Own-child   \n",
       "19    Married-civ-spouse       Craft-repair         Husband   \n",
       "20              Divorced     Prof-specialty   Not-in-family   \n",
       "21    Married-civ-spouse       Craft-repair         Husband   \n",
       "22         Never-married  Handlers-cleaners       Own-child   \n",
       "23    Married-civ-spouse   Transport-moving         Husband   \n",
       "24         Never-married      Other-service       Unmarried   \n",
       "25    Married-civ-spouse              Sales         Husband   \n",
       "26         Never-married   Transport-moving       Own-child   \n",
       "27              Divorced              Sales       Own-child   \n",
       "28         Never-married       Adm-clerical   Not-in-family   \n",
       "29         Never-married       Craft-repair  Other-relative   \n",
       "...                  ...                ...             ...   \n",
       "2412  Married-civ-spouse       Craft-repair         Husband   \n",
       "2413  Married-civ-spouse       Craft-repair         Husband   \n",
       "2414  Married-civ-spouse              Sales         Husband   \n",
       "2415       Never-married      Other-service   Not-in-family   \n",
       "2416            Divorced    Exec-managerial   Not-in-family   \n",
       "2417       Never-married     Prof-specialty   Not-in-family   \n",
       "2418  Married-civ-spouse     Prof-specialty         Husband   \n",
       "2419  Married-civ-spouse      Other-service         Husband   \n",
       "2420  Married-civ-spouse              Sales         Husband   \n",
       "2421  Married-civ-spouse    Exec-managerial         Husband   \n",
       "2422            Divorced    Farming-fishing       Unmarried   \n",
       "2423  Married-civ-spouse     Prof-specialty         Husband   \n",
       "2424  Married-civ-spouse              Sales         Husband   \n",
       "2425  Married-civ-spouse              Sales         Husband   \n",
       "2426  Married-civ-spouse              Sales         Husband   \n",
       "2427             Widowed  Machine-op-inspct   Not-in-family   \n",
       "2428           Separated       Craft-repair       Unmarried   \n",
       "2429  Married-civ-spouse    Exec-managerial         Husband   \n",
       "2430  Married-civ-spouse              Sales         Husband   \n",
       "2431  Married-civ-spouse   Transport-moving         Husband   \n",
       "2432  Married-civ-spouse    Exec-managerial         Husband   \n",
       "2433  Married-civ-spouse    Exec-managerial         Husband   \n",
       "2434            Divorced       Adm-clerical   Not-in-family   \n",
       "2435       Never-married  Handlers-cleaners       Own-child   \n",
       "2436  Married-civ-spouse       Craft-repair         Husband   \n",
       "2437       Never-married       Adm-clerical   Not-in-family   \n",
       "2438  Married-civ-spouse              Sales         Husband   \n",
       "2439       Never-married    Farming-fishing       Own-child   \n",
       "2440       Never-married    Exec-managerial       Unmarried   \n",
       "2441  Married-civ-spouse     Prof-specialty         Husband   \n",
       "\n",
       "                    race     sex  lcapitalgain  lcapitalloss  hoursperweek  \\\n",
       "0                  Black  Female      0.000000      0.000000            40   \n",
       "1                  White    Male      0.000000      0.000000            48   \n",
       "2                  White    Male      0.000000      0.000000            50   \n",
       "3                  White    Male      0.000000      0.000000            40   \n",
       "4                  White    Male      0.000000      0.000000            40   \n",
       "5                  White    Male      0.000000      0.000000            60   \n",
       "6                  White    Male      0.000000      0.000000            60   \n",
       "7                  White    Male      0.000000      0.000000            40   \n",
       "8                  White  Female      0.000000      0.000000            25   \n",
       "9                  White    Male      0.000000      0.000000            40   \n",
       "10                 White  Female      0.000000      0.000000            15   \n",
       "11                 Black  Female      0.000000      0.000000            40   \n",
       "12                 White  Female      0.000000      7.601902            40   \n",
       "13                 White    Male      0.000000      0.000000            40   \n",
       "14                 Black    Male      0.000000      0.000000            40   \n",
       "15                 White    Male      0.000000      0.000000            47   \n",
       "16                 White    Male      0.000000      0.000000            45   \n",
       "17                 White    Male      0.000000      0.000000            45   \n",
       "18                 White  Female      0.000000      0.000000            48   \n",
       "19                 White    Male      0.000000      0.000000            70   \n",
       "20                 White  Female      0.000000      0.000000            36   \n",
       "21                 White    Male      0.000000      0.000000            40   \n",
       "22                 White    Male      0.000000      0.000000            10   \n",
       "23                 White    Male      0.000000      0.000000            50   \n",
       "24                 White    Male      0.000000      0.000000            35   \n",
       "25                 White    Male      0.000000      0.000000            70   \n",
       "26                 White    Male      0.000000      0.000000            40   \n",
       "27                 White  Female      0.000000      0.000000            40   \n",
       "28                 White    Male      0.000000      0.000000            40   \n",
       "29                 White  Female      0.000000      0.000000            15   \n",
       "...                  ...     ...           ...           ...           ...   \n",
       "2412               White    Male      0.000000      0.000000            40   \n",
       "2413               White    Male      0.000000      0.000000            40   \n",
       "2414               White    Male      0.000000      7.138073            40   \n",
       "2415               White  Female      0.000000      0.000000            55   \n",
       "2416               White    Male      0.000000      0.000000            60   \n",
       "2417               White    Male      0.000000      0.000000            45   \n",
       "2418               White    Male      0.000000      0.000000            50   \n",
       "2419               White    Male      0.000000      0.000000            40   \n",
       "2420               White    Male      0.000000      0.000000            50   \n",
       "2421               White    Male      0.000000      0.000000            40   \n",
       "2422               White    Male      0.000000      0.000000            48   \n",
       "2423               White    Male      0.000000      0.000000            40   \n",
       "2424               White    Male      0.000000      0.000000            40   \n",
       "2425               White    Male      0.000000      0.000000            20   \n",
       "2426               White    Male      0.000000      0.000000            45   \n",
       "2427               White  Female      0.000000      0.000000            40   \n",
       "2428               White    Male      0.000000      0.000000            40   \n",
       "2429               White    Male      0.000000      0.000000            55   \n",
       "2430               White    Male      0.000000      0.000000            70   \n",
       "2431               White    Male      0.000000      0.000000            25   \n",
       "2432               White    Male      0.000000      0.000000            50   \n",
       "2433               White    Male      0.000000      0.000000            44   \n",
       "2434               White  Female      0.000000      0.000000            35   \n",
       "2435  Amer-Indian-Eskimo    Male      0.000000      0.000000            30   \n",
       "2436               Other    Male      0.000000      0.000000            40   \n",
       "2437               White  Female      0.000000      0.000000            40   \n",
       "2438               White    Male      0.000000      0.000000            42   \n",
       "2439               White    Male      0.000000      0.000000            66   \n",
       "2440  Asian-Pac-Islander    Male      0.000000      0.000000            40   \n",
       "2441               White    Male      9.617471      0.000000            50   \n",
       "\n",
       "      nativecountry income  \n",
       "0     United-States  <=50K  \n",
       "1     United-States   >50K  \n",
       "2     United-States  <=50K  \n",
       "3     United-States  <=50K  \n",
       "4     United-States  <=50K  \n",
       "5     United-States  <=50K  \n",
       "6     United-States   >50K  \n",
       "7     United-States  <=50K  \n",
       "8     United-States  <=50K  \n",
       "9     United-States  <=50K  \n",
       "10    United-States  <=50K  \n",
       "11    United-States  <=50K  \n",
       "12    United-States  <=50K  \n",
       "13    United-States  <=50K  \n",
       "14    United-States  <=50K  \n",
       "15    United-States   >50K  \n",
       "16    United-States  <=50K  \n",
       "17    United-States   >50K  \n",
       "18    United-States  <=50K  \n",
       "19    United-States  <=50K  \n",
       "20    United-States  <=50K  \n",
       "21      El-Salvador  <=50K  \n",
       "22    United-States  <=50K  \n",
       "23    United-States  <=50K  \n",
       "24           Mexico  <=50K  \n",
       "25    United-States  <=50K  \n",
       "26    United-States  <=50K  \n",
       "27    United-States  <=50K  \n",
       "28    United-States  <=50K  \n",
       "29    United-States  <=50K  \n",
       "...             ...    ...  \n",
       "2412  United-States   >50K  \n",
       "2413         Mexico  <=50K  \n",
       "2414  United-States  <=50K  \n",
       "2415  United-States  <=50K  \n",
       "2416  United-States   >50K  \n",
       "2417  United-States   >50K  \n",
       "2418  United-States   >50K  \n",
       "2419  United-States  <=50K  \n",
       "2420  United-States  <=50K  \n",
       "2421  United-States   >50K  \n",
       "2422  United-States  <=50K  \n",
       "2423  United-States   >50K  \n",
       "2424  United-States  <=50K  \n",
       "2425  United-States   >50K  \n",
       "2426  United-States   >50K  \n",
       "2427  United-States  <=50K  \n",
       "2428  United-States  <=50K  \n",
       "2429  United-States   >50K  \n",
       "2430  United-States  <=50K  \n",
       "2431  United-States   >50K  \n",
       "2432  United-States   >50K  \n",
       "2433  United-States   >50K  \n",
       "2434           Cuba  <=50K  \n",
       "2435  United-States  <=50K  \n",
       "2436         Mexico  <=50K  \n",
       "2437  United-States  <=50K  \n",
       "2438  United-States   >50K  \n",
       "2439  United-States  <=50K  \n",
       "2440          South  <=50K  \n",
       "2441  United-States   >50K  \n",
       "\n",
       "[2442 rows x 15 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [i for i, dt in enumerate(mydata.data.dtypes.values) if dt.name == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [i for i, dt in enumerate(mydata.data.dtypes.values)]\n",
    "categoricals = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_dict = OneHotEncoder(categories='auto').fit(mydata.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 87, 88, 90],\n",
       "       dtype=object),\n",
       " array(['Federal-gov', 'Local-gov', 'Private', 'Self-emp-inc',\n",
       "        'Self-emp-not-inc', 'State-gov', 'Unknown', 'Without-pay'],\n",
       "       dtype=object),\n",
       " array([10.384152893207228, 10.404686992907486, 10.449612750289052, ...,\n",
       "        13.615353286233153, 13.676039273344056, 13.735042093002296],\n",
       "       dtype=object),\n",
       " array(['10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th',\n",
       "        'Assoc-acdm', 'Assoc-voc', 'Bachelors', 'Doctorate', 'HS-grad',\n",
       "        'Masters', 'Preschool', 'Prof-school', 'Some-college'],\n",
       "       dtype=object),\n",
       " array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
       "       dtype=object),\n",
       " array(['Divorced', 'Married-AF-spouse', 'Married-civ-spouse',\n",
       "        'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'],\n",
       "       dtype=object),\n",
       " array(['?', 'Adm-clerical', 'Armed-Forces', 'Craft-repair',\n",
       "        'Exec-managerial', 'Farming-fishing', 'Handlers-cleaners',\n",
       "        'Machine-op-inspct', 'Other-service', 'Priv-house-serv',\n",
       "        'Prof-specialty', 'Protective-serv', 'Sales', 'Tech-support',\n",
       "        'Transport-moving'], dtype=object),\n",
       " array(['Husband', 'Not-in-family', 'Other-relative', 'Own-child',\n",
       "        'Unmarried', 'Wife'], dtype=object),\n",
       " array(['Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other',\n",
       "        'White'], dtype=object),\n",
       " array(['Female', 'Male'], dtype=object),\n",
       " array([0.0, 6.38856140554563, 6.818924065275521, 6.962243464266208,\n",
       "        7.049254841255838, 7.142827401161621, 7.261927092702751,\n",
       "        7.317876198626496, 7.522400231387126, 7.684783943522785,\n",
       "        7.685703061234548, 7.697575346802343, 7.760893195851023,\n",
       "        7.764296006450517, 7.7865518064287125, 7.789454566086673,\n",
       "        7.809541324653409, 7.862497197230545, 7.967626739333816,\n",
       "        7.975220838653411, 7.999007213243956, 8.04044688130311,\n",
       "        8.051340933292979, 8.109525659752872, 8.137103389639302,\n",
       "        8.148156439921625, 8.150467911624004, 8.209308411646937,\n",
       "        8.238008249218401, 8.271036865792954, 8.310169021981912,\n",
       "        8.319229938632326, 8.386400901166214, 8.393216011596527,\n",
       "        8.413830678421084, 8.444837529224097, 8.473868066677865,\n",
       "        8.490027523343468, 8.504107951867582, 8.519989278718239,\n",
       "        8.552367266423891, 8.604471199523298, 8.767017621311778,\n",
       "        8.779249716229046, 8.781862489558947, 8.796792687674662,\n",
       "        8.813438494528508, 8.895492631451633, 8.915163617762142,\n",
       "        8.947546015032179, 9.06126014896203, 9.147081032336992,\n",
       "        9.261128538808288, 9.269175157697076, 9.514215624322718,\n",
       "        9.571156728415463, 9.617204500998056, 9.617470759403409,\n",
       "        9.669788487282403, 9.906084178383392, 10.136066450636825,\n",
       "        10.233833921427351, 10.62888408731157, 11.512925464970227],\n",
       "       dtype=object),\n",
       " array([0.0, 6.439350371100098, 6.996681488176539, 7.138073034044348,\n",
       "        7.201170883281678, 7.2305631534092925, 7.25063551189868,\n",
       "        7.265429723253952, 7.3038432252777055, 7.3556411029742526,\n",
       "        7.365180126021014, 7.372118028337788, 7.379632152609552,\n",
       "        7.395721608602044, 7.409741954080922, 7.4223737009868245,\n",
       "        7.450079569807499, 7.451241684987676, 7.462214939768188,\n",
       "        7.4627891574124465, 7.5202345564746285, 7.522400231387126,\n",
       "        7.537430036586509, 7.543273346705446, 7.551186867296148,\n",
       "        7.5883236773352225, 7.589841512182658, 7.591357046698551,\n",
       "        7.601901959875167, 7.602401335665817, 7.622174594817622,\n",
       "        7.62657020629066, 7.679713639966372, 7.687080155783135,\n",
       "        7.698936199813447, 7.7226775164680035, 7.757906208351747,\n",
       "        7.780303087908372, 7.789868559054706, 7.820037989458753],\n",
       "       dtype=object),\n",
       " array([1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "        21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39,\n",
       "        40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
       "        57, 58, 60, 65, 66, 68, 70, 72, 75, 77, 80, 84, 90, 98, 99],\n",
       "       dtype=object),\n",
       " array(['Cambodia', 'Canada', 'China', 'Columbia', 'Cuba',\n",
       "        'Dominican-Republic', 'Ecuador', 'El-Salvador', 'England',\n",
       "        'France', 'Germany', 'Greece', 'Guatemala', 'Haiti', 'Honduras',\n",
       "        'Hong', 'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan',\n",
       "        'Laos', 'Mexico', 'Nicaragua', 'Peru', 'Philippines', 'Poland',\n",
       "        'Portugal', 'Puerto-Rico', 'South', 'Taiwan', 'Thailand',\n",
       "        'Trinidad and Tobago', 'United-States', 'Unknown', 'Vietnam',\n",
       "        'Yugoslavia'], dtype=object),\n",
       " array(['<=50K', '>50K'], dtype=object)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_dict.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_17', 'x0_18', 'x0_19', ..., 'x13_Yugoslavia', 'x14_<=50K',\n",
       "       'x14_>50K'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_dict.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['Federal-gov' 'Private' 'Private' ... 'Private' 'Private' 'Self-emp-inc'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-52370df71143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moh_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmydata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'workclass'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    611\u001b[0m                                        copy=True)\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m         \u001b[0mX_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\B3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['Federal-gov' 'Private' 'Private' ... 'Private' 'Private' 'Self-emp-inc'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "oh_dict.transform(mydata.data['workclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Federal-gov', 'Local-gov', 'Private', 'Self-emp-inc',\n",
       "       'Self-emp-not-inc', 'State-gov', 'Unknown', 'Without-pay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_dict.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 2, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_dict.transform(mydata.data['workclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LabelEncoder' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6b445b672d37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mle_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LabelEncoder' object has no attribute 'keys'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
