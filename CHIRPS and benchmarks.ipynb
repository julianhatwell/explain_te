{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prologue\n",
    "import forest_surveyor.reproducible as rp\n",
    "# from forest_surveyor.experiments import grid_experiment_mp\n",
    "# from forest_surveyor.routines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function forest_surveyor.datasets.adult_small_samp_data>,\n",
       " <function forest_surveyor.datasets.bankmark_samp_data>,\n",
       " <function forest_surveyor.datasets.car_data>,\n",
       " <function forest_surveyor.datasets.cardio_data>,\n",
       " <function forest_surveyor.datasets.credit_data>,\n",
       " <function forest_surveyor.datasets.german_data>,\n",
       " <function forest_surveyor.datasets.lending_tiny_samp_data>,\n",
       " <function forest_surveyor.datasets.nursery_samp_data>,\n",
       " <function forest_surveyor.datasets.rcdv_samp_data>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# several datasets are available as pre-prepared containers\n",
    "# containers hold the data and some meta-data\n",
    "# it saves time to pre-process this meta-data\n",
    "# any dataset can be turned into a container by invoking the constructor\n",
    "# the constructor is found in the file structures.py\n",
    "\n",
    "# this is a list of constructors with pre-configured parameters\n",
    "# and data that is stored locally in the package\n",
    "rp.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for now\n",
    "dataset = datasets[0]\n",
    "project_dir = 'V:\\\\whiteboxing'\n",
    "random_state = 123\n",
    "override_tuning = True\n",
    "add_trees = 0\n",
    "eval_model = False\n",
    "\n",
    "# general data preparation\n",
    "mydata = rp.datasets[0](random_state=random_state, project_dir=project_dir)\n",
    "\n",
    "# train test split - one off hard-coded\n",
    "# the random_state is not used here. We vary the random state on each forest\n",
    "train_index, test_index = mydata.get_tt_split_idx(random_state=123)\n",
    "tt = mydata.tt_split_by_idx(train_index, test_index).to_dict()\n",
    "# save these for use in R and G-REX\n",
    "Series(train_index).to_csv(path = mydata.get_save_path() + 'train_index.csv', index=False)\n",
    "Series(test_index).to_csv(path = mydata.get_save_path() + 'test_index.csv', index=False)\n",
    "\n",
    "################ PARAMETER TUNING ###################\n",
    "############ Only runs when required ################\n",
    "#####################################################\n",
    "\n",
    "best_params = tune_rf(tt['X_train_enc'], tt['y_train'],\n",
    " save_path = mydata.get_save_path(),\n",
    " random_state=mydata.random_state,\n",
    " override_tuning=override_tuning)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# update best params according to expermental design\n",
    "best_params['n_estimators'] = best_params['n_estimators'] + add_trees\n",
    "\n",
    "# train a rf model\n",
    "rf, enc_rf = train_rf(X=tt['X_train_enc'], y=tt['y_train'],\n",
    " best_params=best_params,\n",
    " encoder=tt['encoder'],\n",
    " random_state=mydata.random_state)\n",
    "\n",
    "if eval_model:\n",
    "    cm, acc, coka, prfs = evaluate_model(prediction_model=enc_rf, X=tt['X_test'], y=tt['y_test'],\n",
    "                 class_names=mydata.get_label(mydata.class_col, [i for i in range(len(mydata.class_names))]).tolist(),\n",
    "                 plot_cm=True, plot_cm_norm=True)\n",
    "else:\n",
    "    cm, acc, coka, prfs = evaluate_model(prediction_model=enc_rf, X=tt['X_test'], y=tt['y_test'],\n",
    "                 class_names=mydata.get_label(mydata.class_col, [i for i in range(len(mydata.class_names))]).tolist(),\n",
    "                 plot_cm=False, plot_cm_norm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.tile(np.repeat([i + 123 for i in range(30)], len(datasets)), 180)\n",
    "add_trees = np.repeat([i * 200 for i in range(6)], len(datasets) * 30)\n",
    "override_tuning = np.concatenate((np.tile(np.array([True]), 30 * len(datasets)), np.tile(np.array([False]), 5 * 30 * len(datasets))))\n",
    "alpha_scores = np.tile([0.5], 180 * len(datasets))\n",
    "alpha_paths = np.tile([0.5], 180 * len(datasets))\n",
    "support_paths = np.tile([0.05], 180 * len(datasets))\n",
    "run_anchors = np.tile([False], 180 * len(datasets))\n",
    "precis_threshold = np.tile([0.95], 180 * len(datasets))\n",
    "n_instances = np.tile([500], 180 * len(datasets))\n",
    "n_batches = np.tile([1], 180 * len(datasets))\n",
    "eval_model = np.tile([False], 180 * len(datasets))\n",
    "which_trees = np.tile(['majority'], 180 * len(datasets))\n",
    "disc_path_bins = np.tile([4], 180 * len(datasets))\n",
    "disc_path_eqcounts = np.tile([False], 180 * len(datasets))\n",
    "iv_low = np.tile([random_state.min()], 180 * len(datasets))\n",
    "iv_high = np.tile([random_state.max() + 1], 180 * len(datasets))\n",
    "weighting = np.tile(['chisq'], 180 * len(datasets))\n",
    "greedy = np.tile(['precision'], 180 * len(datasets))\n",
    "forest_walk_async = np.tile([True], 180 * len(datasets))\n",
    "chirps_explanation_async = np.tile([True], 180 * len(datasets))\n",
    "project_dir = np.tile(['V:\\\\whiteboxing'], 180 * len(datasets))\n",
    "save_rule_accs = np.tile([False], 180 * len(datasets))\n",
    "\n",
    "datasets = np.tile(datasets, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]\n",
    "project_dir = 'V:\\\\whiteboxing'\n",
    "random_state = 123\n",
    "override_tuning = True\n",
    "add_trees = 0\n",
    "eval_model = False\n",
    "\n",
    "# general data preparation\n",
    "mydata = dataset(random_state=random_state, project_dir=project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs as mkdir\n",
    "from pathlib import Path as pth\n",
    "pth(mydata.get_save_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pth(mydata.get_save_path()).is_dir():\n",
    "    mkdir(mydata.get_save_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
